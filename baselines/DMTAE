import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from data_preprocess import Dataset as Dataset
# from autoencoder import Autoencoder
import numpy as np
import matplotlib.pyplot as plt
from utils import classify_with_knn
import pandas as pd

##
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

epochs = 300
batch_size = 200
feats = 20
domains = 10
classes = 2
latent_dim = 15
learning_rate = 0.005

# model = Autoencoder().to(device)
criterion = nn.MSELoss()


class MultitaskAutoencoder(nn.Module):
    def __init__(self, D_in, H=25, H2=20, latent_dim=15):
        # Encoder
        super(MultitaskAutoencoder, self).__init__()
        self.linear1 = nn.Linear(D_in, H)  # 29 * 20
        self.lin_bn1 = nn.BatchNorm1d(num_features=H)
        self.linear2 = nn.Linear(H, H2)  # 20 * 10
        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)
        self.linear3 = nn.Linear(H2, H2)  # 10 * 10
        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)
        self.num_class = classes

        self.fc1 = nn.Linear(H2, latent_dim)  # 10 * 7

        self.classifier = nn.Linear(latent_dim, self.num_class)  # 7 * 3

        # classifier
        # self.classifier = nn.Linear(latent_dim, self.num_class) # 7 * 3

        #         # Decoder
        self.fc3 = nn.Linear(latent_dim, latent_dim)  # 7 * 7
        #         self.fc_bn3 = nn.BatchNorm1d(latent_dim)
        self.fc4 = nn.Linear(latent_dim, H2)  # 7 * 10
        #         self.fc_bn4 = nn.BatchNorm1d(H2)

        self.linear4 = nn.Linear(H2, H2)  # 10 * 10
        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)
        self.linear5 = nn.Linear(H2, H)  # 10 * 20
        self.lin_bn5 = nn.BatchNorm1d(num_features=H)
        self.linear6 = nn.Linear(H, D_in)  # 20 * 29
        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)
        self.relu = nn.ReLU()

    def encode(self, x):
        lin1 = self.relu(self.lin_bn1(self.linear1(x)))  # 29 * 20
        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))  # 20 * 10
        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))  # 10 * 10

        fc1 = self.relu(self.fc1(lin3))  # 10 * 7
        # fc2 = F.relu(self.classifier(fc1)) # 7 * 3
        return fc1

    def decode(self, z):
        fc3 = self.relu(self.fc3(z))  # 7 * 7
        fc4 = self.relu(self.fc4(fc3))  # .view(128, -1) # 7 * 10

        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))  # 10 * 10
        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))  # 10 * 20

        return self.lin_bn6(self.linear6(lin5))  # 20 * 29

    def forward(self, inputs):  # batch * feats

        z = self.encode(inputs)  # 29 * 3
        logits = self.classifier(z)
        reconstruction = self.decode(z)

        return logits, reconstruction, z


class customLoss(nn.Module):
    def __init__(self):
        super(customLoss, self).__init__()
        self.mse_loss = nn.MSELoss()  # (reduction="sum")
        self.classification_criterion = nn.CrossEntropyLoss()

    def forward(self, input, dec_out, Z, dom_out, logits, targets):
        loss_MSE = self.mse_loss(dec_out, dom_out)
        # print ("LOSS MSE:", loss_MSE)

        targets = torch.flatten(targets)
        # print ("TARGETS:",targets)

        classification_loss = self.classification_criterion(logits, targets)

        #entropy_loss = self.entropy_loss(input, Z, True)

        total_loss = loss_MSE + classification_loss

        #total_loss = classification_loss + (0.9 * loss_MSE) #+ (2 * entropy_loss)

        # return classification_loss + (0.5 * loss_MSE) + (1 * entropy_loss)
        return total_loss


# D_in = data_set.x.shape[1]
D_in = 29
print("D_in shape:", D_in)
H = 25
H2 = 20

log_interval = 150
val_losses = []
train_losses = []

multitaskAE = MultitaskAutoencoder(D_in, H, H2).to(device)
optimizer = torch.optim.Adam(multitaskAE.parameters(), lr=learning_rate, weight_decay=1e-5)


def train_AE():
    for epoch in range(epochs):
        train_loss = 0
        losses = []

        for domain in range(domains):
            dataset = Dataset(dom=domain)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

            for input, out, targets in dataloader:
                # data = data.to(device)

                input = input.float().to(device)
                # print ("INPUTS SHAPE:", input.shape)

                dom_out = out.float().to(device)  # ground truth data
                # print("OUTS SHAPE:", dom_out.shape)

                targets = targets.long().to(device)
                # print ("TARGETS IN TRAIN:", targets)

                optimizer.zero_grad()

                logits, dec_out, Z = multitaskAE(input)  ## model
                # print ("logits train:", logits)

                loss_mse = customLoss()
                loss = loss_mse(input, dec_out, Z, dom_out, logits, targets)

                losses.append(loss)

        final_loss = torch.mean(torch.stack(losses))
        # print("FINAL LOSS:", final_loss)

        final_loss.backward()
        train_loss += final_loss.item()
        optimizer.step()

        if epochs % 1 == 50:
            print('====> Epoch: {} Average loss: {:.4f}'.format(
                epochs, train_loss))  # / len(dataloader.dataset)))
            train_losses.append(train_loss)  # / len(dataloader.dataset))


train_AE()
PATH = "/content/gdrive/MyDrive/OOD_generalization/mate-mi-reg-model/meta_latent_model_MTAE_20K.pth"
torch.save(multitaskAE, PATH)
torch.save(multitaskAE.state_dict(), PATH)
print("MODEL SAVED")

# print ("TRAIN LOSS:", train_losses)
print("PLOTTING TRAINING:")
X = np.arange(epochs)
Y = train_losses
plt.plot(X, Y)
plt.savefig('loss_vs_epoch.png')


#
#




