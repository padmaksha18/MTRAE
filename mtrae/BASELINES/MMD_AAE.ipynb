{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C19qic_Hygck",
        "outputId": "b4f7ce40-5e8b-4d61-ca79-708244c09927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bXUyIi4x3gMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89146e12-a545-4aac-a37c-69dde913d737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/DATASETS/ARRYTHMIA\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/My Drive/DATASETS/ARRYTHMIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5ED6MSGv4AvS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "#from autoencoder import Autoencoder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data.dataset import Dataset as dataset\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ldNOyaS_LfPF"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('TRAIN_DATA_NORM_VEB_16K.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RQFh9FVxrwW7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "ae5882fe-07b4-47eb-8103-a5f3713da1ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    0_pPeak   0_tPeak   0_rPeak   0_sPeak   0_qPeak  0_qrs_morph0  \\\n",
              "0  0.438303  0.266989  0.576403  0.408554  0.542624      0.542624   \n",
              "1  0.491285  0.258393  0.535019  0.388107  0.550598      0.550598   \n",
              "2  0.440104  0.391836  0.559018  0.317185  0.545932      0.545932   \n",
              "3  0.486867  0.464057  0.670241  0.285911  0.573835      0.573835   \n",
              "4  0.474577  0.342478  0.424945  0.412417  0.565465      0.565465   \n",
              "\n",
              "   0_qrs_morph1  0_qrs_morph3  0_qrs_morph4   1_pPeak   1_tPeak   1_rPeak  \\\n",
              "0      0.488489      0.675400      0.539111  0.292618  0.263815  0.490166   \n",
              "1      0.474882      0.626789      0.615517  0.287513  0.285442  0.478941   \n",
              "2      0.530691      0.635959      0.477287  0.304028  0.325511  0.388111   \n",
              "3      0.601868      0.653305      0.354840  0.302150  0.286775  0.453499   \n",
              "4      0.493817      0.532375      0.540755  0.300088  0.313881  0.518940   \n",
              "\n",
              "    1_sPeak   1_qPeak  1_qrs_morph0  1_qrs_morph1  1_qrs_morph2  1_qrs_morph3  \\\n",
              "0  0.644686  0.577750      0.577750      0.557462      0.620977      0.519430   \n",
              "1  0.642074  0.568453      0.568453      0.547180      0.610081      0.510764   \n",
              "2  0.545082  0.493227      0.493227      0.458583      0.496309      0.399050   \n",
              "3  0.592144  0.547382      0.547382      0.510903      0.559054      0.457303   \n",
              "4  0.675353  0.568931      0.568931      0.551942      0.618632      0.523256   \n",
              "\n",
              "   1_qrs_morph4  Label  \n",
              "0      0.607794      0  \n",
              "1      0.601864      0  \n",
              "2      0.478359      0  \n",
              "3      0.539486      1  \n",
              "4      0.630159      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee39d149-3997-4d4b-a90f-ac18f3ee3405\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_pPeak</th>\n",
              "      <th>0_tPeak</th>\n",
              "      <th>0_rPeak</th>\n",
              "      <th>0_sPeak</th>\n",
              "      <th>0_qPeak</th>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <th>1_pPeak</th>\n",
              "      <th>1_tPeak</th>\n",
              "      <th>1_rPeak</th>\n",
              "      <th>1_sPeak</th>\n",
              "      <th>1_qPeak</th>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.438303</td>\n",
              "      <td>0.266989</td>\n",
              "      <td>0.576403</td>\n",
              "      <td>0.408554</td>\n",
              "      <td>0.542624</td>\n",
              "      <td>0.542624</td>\n",
              "      <td>0.488489</td>\n",
              "      <td>0.675400</td>\n",
              "      <td>0.539111</td>\n",
              "      <td>0.292618</td>\n",
              "      <td>0.263815</td>\n",
              "      <td>0.490166</td>\n",
              "      <td>0.644686</td>\n",
              "      <td>0.577750</td>\n",
              "      <td>0.577750</td>\n",
              "      <td>0.557462</td>\n",
              "      <td>0.620977</td>\n",
              "      <td>0.519430</td>\n",
              "      <td>0.607794</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.491285</td>\n",
              "      <td>0.258393</td>\n",
              "      <td>0.535019</td>\n",
              "      <td>0.388107</td>\n",
              "      <td>0.550598</td>\n",
              "      <td>0.550598</td>\n",
              "      <td>0.474882</td>\n",
              "      <td>0.626789</td>\n",
              "      <td>0.615517</td>\n",
              "      <td>0.287513</td>\n",
              "      <td>0.285442</td>\n",
              "      <td>0.478941</td>\n",
              "      <td>0.642074</td>\n",
              "      <td>0.568453</td>\n",
              "      <td>0.568453</td>\n",
              "      <td>0.547180</td>\n",
              "      <td>0.610081</td>\n",
              "      <td>0.510764</td>\n",
              "      <td>0.601864</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.440104</td>\n",
              "      <td>0.391836</td>\n",
              "      <td>0.559018</td>\n",
              "      <td>0.317185</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.530691</td>\n",
              "      <td>0.635959</td>\n",
              "      <td>0.477287</td>\n",
              "      <td>0.304028</td>\n",
              "      <td>0.325511</td>\n",
              "      <td>0.388111</td>\n",
              "      <td>0.545082</td>\n",
              "      <td>0.493227</td>\n",
              "      <td>0.493227</td>\n",
              "      <td>0.458583</td>\n",
              "      <td>0.496309</td>\n",
              "      <td>0.399050</td>\n",
              "      <td>0.478359</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.486867</td>\n",
              "      <td>0.464057</td>\n",
              "      <td>0.670241</td>\n",
              "      <td>0.285911</td>\n",
              "      <td>0.573835</td>\n",
              "      <td>0.573835</td>\n",
              "      <td>0.601868</td>\n",
              "      <td>0.653305</td>\n",
              "      <td>0.354840</td>\n",
              "      <td>0.302150</td>\n",
              "      <td>0.286775</td>\n",
              "      <td>0.453499</td>\n",
              "      <td>0.592144</td>\n",
              "      <td>0.547382</td>\n",
              "      <td>0.547382</td>\n",
              "      <td>0.510903</td>\n",
              "      <td>0.559054</td>\n",
              "      <td>0.457303</td>\n",
              "      <td>0.539486</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.474577</td>\n",
              "      <td>0.342478</td>\n",
              "      <td>0.424945</td>\n",
              "      <td>0.412417</td>\n",
              "      <td>0.565465</td>\n",
              "      <td>0.565465</td>\n",
              "      <td>0.493817</td>\n",
              "      <td>0.532375</td>\n",
              "      <td>0.540755</td>\n",
              "      <td>0.300088</td>\n",
              "      <td>0.313881</td>\n",
              "      <td>0.518940</td>\n",
              "      <td>0.675353</td>\n",
              "      <td>0.568931</td>\n",
              "      <td>0.568931</td>\n",
              "      <td>0.551942</td>\n",
              "      <td>0.618632</td>\n",
              "      <td>0.523256</td>\n",
              "      <td>0.630159</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee39d149-3997-4d4b-a90f-ac18f3ee3405')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ee39d149-3997-4d4b-a90f-ac18f3ee3405 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ee39d149-3997-4d4b-a90f-ac18f3ee3405');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-99534e9e-d708-45ca-9dc2-78f19cdad401\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-99534e9e-d708-45ca-9dc2-78f19cdad401')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-99534e9e-d708-45ca-9dc2-78f19cdad401 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 16000,\n  \"fields\": [\n    {\n      \"column\": \"0_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03200056910122813,\n        \"min\": 0.3075823877640271,\n        \"max\": 0.76215691549589,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4573004068722703,\n          0.4406361909151436,\n          0.4771633753572261\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09085167747353573,\n        \"min\": 0.0,\n        \"max\": 0.7457004088200956,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4805922706082204,\n          0.3940809743570945,\n          0.4787767196554129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12298732272031346,\n        \"min\": 0.0,\n        \"max\": 0.7656244430251042,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.205123300483784,\n          0.5654224877965995,\n          0.1977068654315215\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11474374610924078,\n        \"min\": 0.0668516174092629,\n        \"max\": 0.7557581539140097,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2665647975689121,\n          0.3100165833484078,\n          0.2398109355416356\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.045119901187188435,\n        \"min\": 0.2155508831400355,\n        \"max\": 0.8680787086889623,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4041418515118228,\n          0.5455542947002998,\n          0.3973231592176082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.045119901187188435,\n        \"min\": 0.2155508831400355,\n        \"max\": 0.8680787086889623,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4041418515118228,\n          0.5455542947002998,\n          0.3973231592176082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07850549638004721,\n        \"min\": 0.0,\n        \"max\": 0.8127222848443608,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2493456163109918,\n          0.5456188740886639,\n          0.2325669191341415\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12673336050988634,\n        \"min\": 0.0,\n        \"max\": 0.8392720715100526,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2362750915422091,\n          0.6282029515634792,\n          0.2096951482426089\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1190398410377523,\n        \"min\": 0.0,\n        \"max\": 0.9670248226626524,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2631368276407591,\n          0.4733453709558748,\n          0.2299288608449821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021373399868941073,\n        \"min\": 0.103296503627264,\n        \"max\": 0.946746586859583,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2888487006991255,\n          0.3024370057450008,\n          0.2922316845191572\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04142871817638884,\n        \"min\": 0.0833424589103086,\n        \"max\": 0.5740994621296992,\n        \"num_unique_values\": 15998,\n        \"samples\": [\n          0.2618493321739159,\n          0.269622228303406,\n          0.3366537212510448\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06054978106459086,\n        \"min\": 0.0954939494491432,\n        \"max\": 0.9754363708418734,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5551284501812946,\n          0.3944340786365414,\n          0.4766448345901409\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05703814506637455,\n        \"min\": 0.1955124228708535,\n        \"max\": 0.9046390075837883,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5828038525092257,\n          0.5419345516851752,\n          0.6401642436322763\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05111127255474416,\n        \"min\": 0.1857001124068941,\n        \"max\": 0.9332491193327254,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5671782455682178,\n          0.4984640376282012,\n          0.5649630633227518\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05111127255474416,\n        \"min\": 0.1857001124068941,\n        \"max\": 0.9332491193327254,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5671782455682178,\n          0.4984640376282012,\n          0.5649630633227518\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06069681634266127,\n        \"min\": 0.1216373762031989,\n        \"max\": 0.945921448366162,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5871251607642702,\n          0.4647507292791503,\n          0.5430691601854578\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07137295774971862,\n        \"min\": 0.1567992832353253,\n        \"max\": 0.9522165932974588,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.6843553042837652,\n          0.5013116913797464,\n          0.6054701879063594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06478898973060594,\n        \"min\": 0.1000934094099245,\n        \"max\": 0.7463755873036746,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5487430513064904,\n          0.4011731392261828,\n          0.5069861382627775\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06879711614864603,\n        \"min\": 0.1777647696134554,\n        \"max\": 0.8713606623616184,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5939617645894224,\n          0.477592058512391,\n          0.5985508152847284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nVdPhpISA3uC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b423a2e-ce87-4d4c-8c5b-2bedc7b233b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_df['Label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Label'][train_df['Label']>1.0] = 1.0"
      ],
      "metadata": {
        "id": "dtAuHTAYUqZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5496222-eed6-4376-8834-910eba26f6ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2336b14ac71e>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['Label'][train_df['Label']>1.0] = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yns7nxwlIY8t",
        "outputId": "d6c992c7-e5d1-440f-9730-e796973f3484"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16000, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Label'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9RQSy_kUvv2",
        "outputId": "e5629236-97d7-4454-a527-f928e6cbad11"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df0 = train_df.iloc[0:4000]\n",
        "#df3.loc[ df3['Label'] == 2.0, 'Label'] = 1.0\n",
        "df0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "PQvYPs0TIino",
        "outputId": "e2dede93-bdeb-49ec-9322-33ef5d8fa36b",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0_pPeak   0_tPeak   0_rPeak   0_sPeak   0_qPeak  0_qrs_morph0  \\\n",
              "0     0.438303  0.266989  0.576403  0.408554  0.542624      0.542624   \n",
              "1     0.491285  0.258393  0.535019  0.388107  0.550598      0.550598   \n",
              "2     0.440104  0.391836  0.559018  0.317185  0.545932      0.545932   \n",
              "3     0.486867  0.464057  0.670241  0.285911  0.573835      0.573835   \n",
              "4     0.474577  0.342478  0.424945  0.412417  0.565465      0.565465   \n",
              "...        ...       ...       ...       ...       ...           ...   \n",
              "3995  0.471422  0.284782  0.477482  0.411876  0.553217      0.553217   \n",
              "3996  0.461224  0.497188  0.533180  0.609526  0.570435      0.570435   \n",
              "3997  0.496278  0.393054  0.448834  0.353799  0.504316      0.504316   \n",
              "3998  0.453347  0.414441  0.452463  0.542009  0.533790      0.533790   \n",
              "3999  0.439488  0.601493  0.625581  0.686815  0.547274      0.547274   \n",
              "\n",
              "      0_qrs_morph1  0_qrs_morph3  0_qrs_morph4   1_pPeak   1_tPeak   1_rPeak  \\\n",
              "0         0.488489      0.675400      0.539111  0.292618  0.263815  0.490166   \n",
              "1         0.474882      0.626789      0.615517  0.287513  0.285442  0.478941   \n",
              "2         0.530691      0.635959      0.477287  0.304028  0.325511  0.388111   \n",
              "3         0.601868      0.653305      0.354840  0.302150  0.286775  0.453499   \n",
              "4         0.493817      0.532375      0.540755  0.300088  0.313881  0.518940   \n",
              "...            ...           ...           ...       ...       ...       ...   \n",
              "3995      0.477100      0.577394      0.581070  0.303281  0.282392  0.483205   \n",
              "3996      0.499744      0.570957      0.691194  0.280828  0.329659  0.409817   \n",
              "3997      0.423059      0.481948      0.625622  0.290551  0.330377  0.502657   \n",
              "3998      0.450149      0.464623      0.543422  0.291453  0.283516  0.491708   \n",
              "3999      0.468370      0.565965      0.718971  0.292435  0.273715  0.499651   \n",
              "\n",
              "       1_sPeak   1_qPeak  1_qrs_morph0  1_qrs_morph1  1_qrs_morph2  \\\n",
              "0     0.644686  0.577750      0.577750      0.557462      0.620977   \n",
              "1     0.642074  0.568453      0.568453      0.547180      0.610081   \n",
              "2     0.545082  0.493227      0.493227      0.458583      0.496309   \n",
              "3     0.592144  0.547382      0.547382      0.510903      0.559054   \n",
              "4     0.675353  0.568931      0.568931      0.551942      0.618632   \n",
              "...        ...       ...           ...           ...           ...   \n",
              "3995  0.645623  0.570174      0.570174      0.549207      0.612575   \n",
              "3996  0.578276  0.511204      0.511204      0.479755      0.529763   \n",
              "3997  0.528398  0.575664      0.575664      0.556837      0.633060   \n",
              "3998  0.639434  0.576995      0.576995      0.557933      0.624273   \n",
              "3999  0.659305  0.568404      0.568404      0.549876      0.618085   \n",
              "\n",
              "      1_qrs_morph3  1_qrs_morph4  Label  \n",
              "0         0.519430      0.607794      0  \n",
              "1         0.510764      0.601864      0  \n",
              "2         0.399050      0.478359      0  \n",
              "3         0.457303      0.539486      1  \n",
              "4         0.523256      0.630159      1  \n",
              "...            ...           ...    ...  \n",
              "3995      0.513024      0.604997      0  \n",
              "3996      0.435593      0.520645      1  \n",
              "3997      0.528170      0.532425      1  \n",
              "3998      0.518735      0.602449      0  \n",
              "3999      0.522989      0.619292      0  \n",
              "\n",
              "[4000 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea6b58ab-3099-4284-b460-bf7103258fab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_pPeak</th>\n",
              "      <th>0_tPeak</th>\n",
              "      <th>0_rPeak</th>\n",
              "      <th>0_sPeak</th>\n",
              "      <th>0_qPeak</th>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <th>1_pPeak</th>\n",
              "      <th>1_tPeak</th>\n",
              "      <th>1_rPeak</th>\n",
              "      <th>1_sPeak</th>\n",
              "      <th>1_qPeak</th>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.438303</td>\n",
              "      <td>0.266989</td>\n",
              "      <td>0.576403</td>\n",
              "      <td>0.408554</td>\n",
              "      <td>0.542624</td>\n",
              "      <td>0.542624</td>\n",
              "      <td>0.488489</td>\n",
              "      <td>0.675400</td>\n",
              "      <td>0.539111</td>\n",
              "      <td>0.292618</td>\n",
              "      <td>0.263815</td>\n",
              "      <td>0.490166</td>\n",
              "      <td>0.644686</td>\n",
              "      <td>0.577750</td>\n",
              "      <td>0.577750</td>\n",
              "      <td>0.557462</td>\n",
              "      <td>0.620977</td>\n",
              "      <td>0.519430</td>\n",
              "      <td>0.607794</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.491285</td>\n",
              "      <td>0.258393</td>\n",
              "      <td>0.535019</td>\n",
              "      <td>0.388107</td>\n",
              "      <td>0.550598</td>\n",
              "      <td>0.550598</td>\n",
              "      <td>0.474882</td>\n",
              "      <td>0.626789</td>\n",
              "      <td>0.615517</td>\n",
              "      <td>0.287513</td>\n",
              "      <td>0.285442</td>\n",
              "      <td>0.478941</td>\n",
              "      <td>0.642074</td>\n",
              "      <td>0.568453</td>\n",
              "      <td>0.568453</td>\n",
              "      <td>0.547180</td>\n",
              "      <td>0.610081</td>\n",
              "      <td>0.510764</td>\n",
              "      <td>0.601864</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.440104</td>\n",
              "      <td>0.391836</td>\n",
              "      <td>0.559018</td>\n",
              "      <td>0.317185</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.530691</td>\n",
              "      <td>0.635959</td>\n",
              "      <td>0.477287</td>\n",
              "      <td>0.304028</td>\n",
              "      <td>0.325511</td>\n",
              "      <td>0.388111</td>\n",
              "      <td>0.545082</td>\n",
              "      <td>0.493227</td>\n",
              "      <td>0.493227</td>\n",
              "      <td>0.458583</td>\n",
              "      <td>0.496309</td>\n",
              "      <td>0.399050</td>\n",
              "      <td>0.478359</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.486867</td>\n",
              "      <td>0.464057</td>\n",
              "      <td>0.670241</td>\n",
              "      <td>0.285911</td>\n",
              "      <td>0.573835</td>\n",
              "      <td>0.573835</td>\n",
              "      <td>0.601868</td>\n",
              "      <td>0.653305</td>\n",
              "      <td>0.354840</td>\n",
              "      <td>0.302150</td>\n",
              "      <td>0.286775</td>\n",
              "      <td>0.453499</td>\n",
              "      <td>0.592144</td>\n",
              "      <td>0.547382</td>\n",
              "      <td>0.547382</td>\n",
              "      <td>0.510903</td>\n",
              "      <td>0.559054</td>\n",
              "      <td>0.457303</td>\n",
              "      <td>0.539486</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.474577</td>\n",
              "      <td>0.342478</td>\n",
              "      <td>0.424945</td>\n",
              "      <td>0.412417</td>\n",
              "      <td>0.565465</td>\n",
              "      <td>0.565465</td>\n",
              "      <td>0.493817</td>\n",
              "      <td>0.532375</td>\n",
              "      <td>0.540755</td>\n",
              "      <td>0.300088</td>\n",
              "      <td>0.313881</td>\n",
              "      <td>0.518940</td>\n",
              "      <td>0.675353</td>\n",
              "      <td>0.568931</td>\n",
              "      <td>0.568931</td>\n",
              "      <td>0.551942</td>\n",
              "      <td>0.618632</td>\n",
              "      <td>0.523256</td>\n",
              "      <td>0.630159</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>0.471422</td>\n",
              "      <td>0.284782</td>\n",
              "      <td>0.477482</td>\n",
              "      <td>0.411876</td>\n",
              "      <td>0.553217</td>\n",
              "      <td>0.553217</td>\n",
              "      <td>0.477100</td>\n",
              "      <td>0.577394</td>\n",
              "      <td>0.581070</td>\n",
              "      <td>0.303281</td>\n",
              "      <td>0.282392</td>\n",
              "      <td>0.483205</td>\n",
              "      <td>0.645623</td>\n",
              "      <td>0.570174</td>\n",
              "      <td>0.570174</td>\n",
              "      <td>0.549207</td>\n",
              "      <td>0.612575</td>\n",
              "      <td>0.513024</td>\n",
              "      <td>0.604997</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>0.461224</td>\n",
              "      <td>0.497188</td>\n",
              "      <td>0.533180</td>\n",
              "      <td>0.609526</td>\n",
              "      <td>0.570435</td>\n",
              "      <td>0.570435</td>\n",
              "      <td>0.499744</td>\n",
              "      <td>0.570957</td>\n",
              "      <td>0.691194</td>\n",
              "      <td>0.280828</td>\n",
              "      <td>0.329659</td>\n",
              "      <td>0.409817</td>\n",
              "      <td>0.578276</td>\n",
              "      <td>0.511204</td>\n",
              "      <td>0.511204</td>\n",
              "      <td>0.479755</td>\n",
              "      <td>0.529763</td>\n",
              "      <td>0.435593</td>\n",
              "      <td>0.520645</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>0.496278</td>\n",
              "      <td>0.393054</td>\n",
              "      <td>0.448834</td>\n",
              "      <td>0.353799</td>\n",
              "      <td>0.504316</td>\n",
              "      <td>0.504316</td>\n",
              "      <td>0.423059</td>\n",
              "      <td>0.481948</td>\n",
              "      <td>0.625622</td>\n",
              "      <td>0.290551</td>\n",
              "      <td>0.330377</td>\n",
              "      <td>0.502657</td>\n",
              "      <td>0.528398</td>\n",
              "      <td>0.575664</td>\n",
              "      <td>0.575664</td>\n",
              "      <td>0.556837</td>\n",
              "      <td>0.633060</td>\n",
              "      <td>0.528170</td>\n",
              "      <td>0.532425</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>0.453347</td>\n",
              "      <td>0.414441</td>\n",
              "      <td>0.452463</td>\n",
              "      <td>0.542009</td>\n",
              "      <td>0.533790</td>\n",
              "      <td>0.533790</td>\n",
              "      <td>0.450149</td>\n",
              "      <td>0.464623</td>\n",
              "      <td>0.543422</td>\n",
              "      <td>0.291453</td>\n",
              "      <td>0.283516</td>\n",
              "      <td>0.491708</td>\n",
              "      <td>0.639434</td>\n",
              "      <td>0.576995</td>\n",
              "      <td>0.576995</td>\n",
              "      <td>0.557933</td>\n",
              "      <td>0.624273</td>\n",
              "      <td>0.518735</td>\n",
              "      <td>0.602449</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>0.439488</td>\n",
              "      <td>0.601493</td>\n",
              "      <td>0.625581</td>\n",
              "      <td>0.686815</td>\n",
              "      <td>0.547274</td>\n",
              "      <td>0.547274</td>\n",
              "      <td>0.468370</td>\n",
              "      <td>0.565965</td>\n",
              "      <td>0.718971</td>\n",
              "      <td>0.292435</td>\n",
              "      <td>0.273715</td>\n",
              "      <td>0.499651</td>\n",
              "      <td>0.659305</td>\n",
              "      <td>0.568404</td>\n",
              "      <td>0.568404</td>\n",
              "      <td>0.549876</td>\n",
              "      <td>0.618085</td>\n",
              "      <td>0.522989</td>\n",
              "      <td>0.619292</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows  20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea6b58ab-3099-4284-b460-bf7103258fab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ea6b58ab-3099-4284-b460-bf7103258fab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ea6b58ab-3099-4284-b460-bf7103258fab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a1ca34b9-567b-4840-9e13-6bfec56661a8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a1ca34b9-567b-4840-9e13-6bfec56661a8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a1ca34b9-567b-4840-9e13-6bfec56661a8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3e6a6c72-6235-47e7-a945-d30912c5d34f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df0')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3e6a6c72-6235-47e7-a945-d30912c5d34f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df0');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df0",
              "summary": "{\n  \"name\": \"df0\",\n  \"rows\": 4000,\n  \"fields\": [\n    {\n      \"column\": \"0_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03204988797893853,\n        \"min\": 0.3453726788780013,\n        \"max\": 0.76215691549589,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4509135722268775,\n          0.5400852969786619,\n          0.4639487221269229\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0893614078558351,\n        \"min\": 0.0532778048301384,\n        \"max\": 0.7457004088200956,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4127200890171358,\n          0.5141555921789436,\n          0.2647672438140135\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12233671807622012,\n        \"min\": 0.0254473789382277,\n        \"max\": 0.7604795899348171,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4518799897267836,\n          0.6660983711737525,\n          0.5153287892399785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11440824815843649,\n        \"min\": 0.0802250977826271,\n        \"max\": 0.7443442398648542,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5415216935993131,\n          0.1725642842007018,\n          0.3930365351382099\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04512767576845818,\n        \"min\": 0.2389472785040209,\n        \"max\": 0.8680787086889623,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5346363642054555,\n          0.482769415623869,\n          0.5496934542783642\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04512767576845818,\n        \"min\": 0.2389472785040209,\n        \"max\": 0.8680787086889623,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5346363642054555,\n          0.482769415623869,\n          0.5496934542783642\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07777339984465768,\n        \"min\": 0.0229264157495801,\n        \"max\": 0.8127222848443608,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4543134227373246,\n          0.4203225620830141,\n          0.4834636420130985\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1248775413264502,\n        \"min\": 0.014604337481679,\n        \"max\": 0.8277103149039106,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4676869824512865,\n          0.7818849477923961,\n          0.6149103897672654\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11811071556843998,\n        \"min\": 0.0162745697431593,\n        \"max\": 0.9591110793119956,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5506840109928894,\n          0.6234869828081346,\n          0.626315681817638\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021672240847384783,\n        \"min\": 0.1896103206527492,\n        \"max\": 0.806961687655384,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.2894421234635558,\n          0.3202595206607981,\n          0.3079944191130769\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04227988381988087,\n        \"min\": 0.1128916541847364,\n        \"max\": 0.5569837579747534,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.2916218523735174,\n          0.4925423803575244,\n          0.3419056053453263\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06098039544378684,\n        \"min\": 0.0954939494491432,\n        \"max\": 0.7764893396455259,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4893151233913546,\n          0.3679441034663973,\n          0.4098429562674259\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05753424618372576,\n        \"min\": 0.3104345690736005,\n        \"max\": 0.8860210924651055,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.6345162451992261,\n          0.5497278458617924,\n          0.5671081489019516\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05157816813330742,\n        \"min\": 0.1857001124068941,\n        \"max\": 0.6752000574777,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5753446158854729,\n          0.2859372453463574,\n          0.5112258295306826\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05157816813330742,\n        \"min\": 0.1857001124068941,\n        \"max\": 0.6752000574777,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5753446158854729,\n          0.2859372453463574,\n          0.5112258295306826\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06092269058671569,\n        \"min\": 0.1216373762031989,\n        \"max\": 0.6758682119924042,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5563319990535044,\n          0.2360586297790344,\n          0.4729619364896844\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07184596226970691,\n        \"min\": 0.1567992832353253,\n        \"max\": 0.9335621574251678,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.6210552751257469,\n          0.3055026932446359,\n          0.5189461346995953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06522392506242963,\n        \"min\": 0.1000934094099245,\n        \"max\": 0.6791709871824947,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5155903899430483,\n          0.2860113087446729,\n          0.4237168356766809\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0690638795559681,\n        \"min\": 0.1777647696134554,\n        \"max\": 0.8484739102475164,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5962769124955503,\n          0.4219282090866654,\n          0.5067574603492195\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = train_df.iloc[4000:8000]\n",
        "#df3.loc[ df3['Label'] == 2.0, 'Label'] = 1.0\n",
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "3sCAx0FpIfKG",
        "outputId": "c32a8d6c-fbaa-48b2-8bee-3e99ae5b444e",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0_pPeak   0_tPeak   0_rPeak   0_sPeak   0_qPeak  0_qrs_morph0  \\\n",
              "4000  0.448411  0.322971  0.434544  0.397068  0.536153      0.536153   \n",
              "4001  0.500057  0.353796  0.311492  0.336963  0.501937      0.501937   \n",
              "4002  0.452067  0.308434  0.447766  0.395428  0.533638      0.533638   \n",
              "4003  0.452811  0.296686  0.415013  0.407628  0.562164      0.562164   \n",
              "4004  0.435052  0.246036  0.664860  0.341554  0.538016      0.538016   \n",
              "...        ...       ...       ...       ...       ...           ...   \n",
              "7995  0.429500  0.252940  0.651155  0.328442  0.538782      0.538782   \n",
              "7996  0.430195  0.306118  0.457913  0.366984  0.539088      0.539088   \n",
              "7997  0.440882  0.398306  0.569082  0.301764  0.545578      0.545578   \n",
              "7998  0.441843  0.390747  0.543400  0.322194  0.551166      0.551166   \n",
              "7999  0.451771  0.380380  0.316850  0.421736  0.506864      0.506864   \n",
              "\n",
              "      0_qrs_morph1  0_qrs_morph3  0_qrs_morph4   1_pPeak   1_tPeak   1_rPeak  \\\n",
              "4000      0.454652      0.500002      0.597734  0.289395  0.285135  0.491049   \n",
              "4001      0.383780      0.352803      0.378772  0.286382  0.302396  0.422881   \n",
              "4002      0.454421      0.493575      0.610962  0.289399  0.293712  0.487347   \n",
              "4003      0.486562      0.526532      0.534420  0.289370  0.376967  0.513188   \n",
              "4004      0.526980      0.743457      0.540774  0.290463  0.286907  0.475718   \n",
              "...            ...           ...           ...       ...       ...       ...   \n",
              "7995      0.484654      0.767613      0.543024  0.300302  0.276464  0.469411   \n",
              "7996      0.463412      0.564496      0.555254  0.290670  0.284260  0.486639   \n",
              "7997      0.539325      0.623748      0.437763  0.303154  0.327967  0.391224   \n",
              "7998      0.534658      0.598551      0.444352  0.301282  0.318864  0.405241   \n",
              "7999      0.412467      0.419689      0.470666  0.287041  0.350924  0.437267   \n",
              "\n",
              "       1_sPeak   1_qPeak  1_qrs_morph0  1_qrs_morph1  1_qrs_morph2  \\\n",
              "4000  0.634333  0.575427      0.575427      0.556932      0.623541   \n",
              "4001  0.595434  0.521869      0.521869      0.492316      0.547557   \n",
              "4002  0.635888  0.575416      0.575416      0.554594      0.614768   \n",
              "4003  0.495777  0.575564      0.575564      0.559003      0.642255   \n",
              "4004  0.639394  0.565785      0.565785      0.544037      0.606499   \n",
              "...        ...       ...           ...           ...           ...   \n",
              "7995  0.634146  0.559004      0.559004      0.536051      0.597398   \n",
              "7996  0.638167  0.574829      0.574829      0.551990      0.612412   \n",
              "7997  0.543217  0.495805      0.495805      0.461619      0.498886   \n",
              "7998  0.554676  0.507414      0.507414      0.475292      0.515051   \n",
              "7999  0.607403  0.460885      0.460885      0.420861      0.482521   \n",
              "\n",
              "      1_qrs_morph3  1_qrs_morph4  Label  \n",
              "4000      0.517434      0.600849      0  \n",
              "4001      0.454118      0.541877      1  \n",
              "4002      0.509994      0.596078      0  \n",
              "4003      0.531997      0.511119      1  \n",
              "4004      0.507519      0.598428      0  \n",
              "...            ...           ...    ...  \n",
              "7995      0.499659      0.590103      0  \n",
              "7996      0.510234      0.598662      0  \n",
              "7997      0.400184      0.477907      0  \n",
              "7998      0.414932      0.493222      0  \n",
              "7999      0.420629      0.534468      0  \n",
              "\n",
              "[4000 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f501972-c44f-4690-8d54-e0bcf6b0754a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_pPeak</th>\n",
              "      <th>0_tPeak</th>\n",
              "      <th>0_rPeak</th>\n",
              "      <th>0_sPeak</th>\n",
              "      <th>0_qPeak</th>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <th>1_pPeak</th>\n",
              "      <th>1_tPeak</th>\n",
              "      <th>1_rPeak</th>\n",
              "      <th>1_sPeak</th>\n",
              "      <th>1_qPeak</th>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4000</th>\n",
              "      <td>0.448411</td>\n",
              "      <td>0.322971</td>\n",
              "      <td>0.434544</td>\n",
              "      <td>0.397068</td>\n",
              "      <td>0.536153</td>\n",
              "      <td>0.536153</td>\n",
              "      <td>0.454652</td>\n",
              "      <td>0.500002</td>\n",
              "      <td>0.597734</td>\n",
              "      <td>0.289395</td>\n",
              "      <td>0.285135</td>\n",
              "      <td>0.491049</td>\n",
              "      <td>0.634333</td>\n",
              "      <td>0.575427</td>\n",
              "      <td>0.575427</td>\n",
              "      <td>0.556932</td>\n",
              "      <td>0.623541</td>\n",
              "      <td>0.517434</td>\n",
              "      <td>0.600849</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4001</th>\n",
              "      <td>0.500057</td>\n",
              "      <td>0.353796</td>\n",
              "      <td>0.311492</td>\n",
              "      <td>0.336963</td>\n",
              "      <td>0.501937</td>\n",
              "      <td>0.501937</td>\n",
              "      <td>0.383780</td>\n",
              "      <td>0.352803</td>\n",
              "      <td>0.378772</td>\n",
              "      <td>0.286382</td>\n",
              "      <td>0.302396</td>\n",
              "      <td>0.422881</td>\n",
              "      <td>0.595434</td>\n",
              "      <td>0.521869</td>\n",
              "      <td>0.521869</td>\n",
              "      <td>0.492316</td>\n",
              "      <td>0.547557</td>\n",
              "      <td>0.454118</td>\n",
              "      <td>0.541877</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4002</th>\n",
              "      <td>0.452067</td>\n",
              "      <td>0.308434</td>\n",
              "      <td>0.447766</td>\n",
              "      <td>0.395428</td>\n",
              "      <td>0.533638</td>\n",
              "      <td>0.533638</td>\n",
              "      <td>0.454421</td>\n",
              "      <td>0.493575</td>\n",
              "      <td>0.610962</td>\n",
              "      <td>0.289399</td>\n",
              "      <td>0.293712</td>\n",
              "      <td>0.487347</td>\n",
              "      <td>0.635888</td>\n",
              "      <td>0.575416</td>\n",
              "      <td>0.575416</td>\n",
              "      <td>0.554594</td>\n",
              "      <td>0.614768</td>\n",
              "      <td>0.509994</td>\n",
              "      <td>0.596078</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4003</th>\n",
              "      <td>0.452811</td>\n",
              "      <td>0.296686</td>\n",
              "      <td>0.415013</td>\n",
              "      <td>0.407628</td>\n",
              "      <td>0.562164</td>\n",
              "      <td>0.562164</td>\n",
              "      <td>0.486562</td>\n",
              "      <td>0.526532</td>\n",
              "      <td>0.534420</td>\n",
              "      <td>0.289370</td>\n",
              "      <td>0.376967</td>\n",
              "      <td>0.513188</td>\n",
              "      <td>0.495777</td>\n",
              "      <td>0.575564</td>\n",
              "      <td>0.575564</td>\n",
              "      <td>0.559003</td>\n",
              "      <td>0.642255</td>\n",
              "      <td>0.531997</td>\n",
              "      <td>0.511119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4004</th>\n",
              "      <td>0.435052</td>\n",
              "      <td>0.246036</td>\n",
              "      <td>0.664860</td>\n",
              "      <td>0.341554</td>\n",
              "      <td>0.538016</td>\n",
              "      <td>0.538016</td>\n",
              "      <td>0.526980</td>\n",
              "      <td>0.743457</td>\n",
              "      <td>0.540774</td>\n",
              "      <td>0.290463</td>\n",
              "      <td>0.286907</td>\n",
              "      <td>0.475718</td>\n",
              "      <td>0.639394</td>\n",
              "      <td>0.565785</td>\n",
              "      <td>0.565785</td>\n",
              "      <td>0.544037</td>\n",
              "      <td>0.606499</td>\n",
              "      <td>0.507519</td>\n",
              "      <td>0.598428</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>0.429500</td>\n",
              "      <td>0.252940</td>\n",
              "      <td>0.651155</td>\n",
              "      <td>0.328442</td>\n",
              "      <td>0.538782</td>\n",
              "      <td>0.538782</td>\n",
              "      <td>0.484654</td>\n",
              "      <td>0.767613</td>\n",
              "      <td>0.543024</td>\n",
              "      <td>0.300302</td>\n",
              "      <td>0.276464</td>\n",
              "      <td>0.469411</td>\n",
              "      <td>0.634146</td>\n",
              "      <td>0.559004</td>\n",
              "      <td>0.559004</td>\n",
              "      <td>0.536051</td>\n",
              "      <td>0.597398</td>\n",
              "      <td>0.499659</td>\n",
              "      <td>0.590103</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>0.430195</td>\n",
              "      <td>0.306118</td>\n",
              "      <td>0.457913</td>\n",
              "      <td>0.366984</td>\n",
              "      <td>0.539088</td>\n",
              "      <td>0.539088</td>\n",
              "      <td>0.463412</td>\n",
              "      <td>0.564496</td>\n",
              "      <td>0.555254</td>\n",
              "      <td>0.290670</td>\n",
              "      <td>0.284260</td>\n",
              "      <td>0.486639</td>\n",
              "      <td>0.638167</td>\n",
              "      <td>0.574829</td>\n",
              "      <td>0.574829</td>\n",
              "      <td>0.551990</td>\n",
              "      <td>0.612412</td>\n",
              "      <td>0.510234</td>\n",
              "      <td>0.598662</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>0.440882</td>\n",
              "      <td>0.398306</td>\n",
              "      <td>0.569082</td>\n",
              "      <td>0.301764</td>\n",
              "      <td>0.545578</td>\n",
              "      <td>0.545578</td>\n",
              "      <td>0.539325</td>\n",
              "      <td>0.623748</td>\n",
              "      <td>0.437763</td>\n",
              "      <td>0.303154</td>\n",
              "      <td>0.327967</td>\n",
              "      <td>0.391224</td>\n",
              "      <td>0.543217</td>\n",
              "      <td>0.495805</td>\n",
              "      <td>0.495805</td>\n",
              "      <td>0.461619</td>\n",
              "      <td>0.498886</td>\n",
              "      <td>0.400184</td>\n",
              "      <td>0.477907</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>0.441843</td>\n",
              "      <td>0.390747</td>\n",
              "      <td>0.543400</td>\n",
              "      <td>0.322194</td>\n",
              "      <td>0.551166</td>\n",
              "      <td>0.551166</td>\n",
              "      <td>0.534658</td>\n",
              "      <td>0.598551</td>\n",
              "      <td>0.444352</td>\n",
              "      <td>0.301282</td>\n",
              "      <td>0.318864</td>\n",
              "      <td>0.405241</td>\n",
              "      <td>0.554676</td>\n",
              "      <td>0.507414</td>\n",
              "      <td>0.507414</td>\n",
              "      <td>0.475292</td>\n",
              "      <td>0.515051</td>\n",
              "      <td>0.414932</td>\n",
              "      <td>0.493222</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>0.451771</td>\n",
              "      <td>0.380380</td>\n",
              "      <td>0.316850</td>\n",
              "      <td>0.421736</td>\n",
              "      <td>0.506864</td>\n",
              "      <td>0.506864</td>\n",
              "      <td>0.412467</td>\n",
              "      <td>0.419689</td>\n",
              "      <td>0.470666</td>\n",
              "      <td>0.287041</td>\n",
              "      <td>0.350924</td>\n",
              "      <td>0.437267</td>\n",
              "      <td>0.607403</td>\n",
              "      <td>0.460885</td>\n",
              "      <td>0.460885</td>\n",
              "      <td>0.420861</td>\n",
              "      <td>0.482521</td>\n",
              "      <td>0.420629</td>\n",
              "      <td>0.534468</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows  20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f501972-c44f-4690-8d54-e0bcf6b0754a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8f501972-c44f-4690-8d54-e0bcf6b0754a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8f501972-c44f-4690-8d54-e0bcf6b0754a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d5102a05-be19-4cf3-8541-c464f487ed88\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d5102a05-be19-4cf3-8541-c464f487ed88')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d5102a05-be19-4cf3-8541-c464f487ed88 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_1185362a-4c48-4da8-b67e-d1030fdad73c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df1')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1185362a-4c48-4da8-b67e-d1030fdad73c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df1');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1",
              "summary": "{\n  \"name\": \"df1\",\n  \"rows\": 4000,\n  \"fields\": [\n    {\n      \"column\": \"0_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03241898726015607,\n        \"min\": 0.3460474793157171,\n        \"max\": 0.7609947869286897,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4400047482645364,\n          0.4344553170372114,\n          0.4484033957481999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08988072497759433,\n        \"min\": 0.0,\n        \"max\": 0.6916880591979493,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.3647259565417813,\n          0.2644458392780438,\n          0.406045377368366\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12364179879922699,\n        \"min\": 0.0800769085306765,\n        \"max\": 0.7582755663295722,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5528006388618538,\n          0.6726435442200431,\n          0.4463882002062952\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11411649401801634,\n        \"min\": 0.1088605814440494,\n        \"max\": 0.7528574658347664,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.422855710721758,\n          0.3354542012680056,\n          0.5369280529526097\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04538084935032245,\n        \"min\": 0.2891738293796959,\n        \"max\": 0.8675493718879795,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5423388032374008,\n          0.539113624252165,\n          0.5340770748687969\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04538084935032245,\n        \"min\": 0.2891738293796959,\n        \"max\": 0.8675493718879795,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5423388032374008,\n          0.539113624252165,\n          0.5340770748687969\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07935761450413659,\n        \"min\": 0.1234630774403572,\n        \"max\": 0.792224058689943,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5585123035257065,\n          0.5158387149409535,\n          0.4542528969618475\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12842077715355346,\n        \"min\": 0.1041264674232788,\n        \"max\": 0.8392720715100526,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4558770648390544,\n          0.7839257866673046,\n          0.4692175101968867\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11966494085272608,\n        \"min\": 0.1111020589491182,\n        \"max\": 0.9670248226626524,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.47335568054614,\n          0.620510040920101,\n          0.5556540766382511\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.020485338589725604,\n        \"min\": 0.103296503627264,\n        \"max\": 0.7029865790970729,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.2901235040941831,\n          0.2848576788225028,\n          0.2905738444448241\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04220037149514068,\n        \"min\": 0.0833424589103086,\n        \"max\": 0.5740994621296992,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.2690233321524778,\n          0.267450513312736,\n          0.2984351546497543\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.061621963651472746,\n        \"min\": 0.1178736368996007,\n        \"max\": 0.8685179477755566,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.4902039915824907,\n          0.485386438189459,\n          0.4909895889976369\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05712948671840027,\n        \"min\": 0.1955124228708535,\n        \"max\": 0.9046390075837883,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.6452179914803343,\n          0.6474370478911535,\n          0.6353242558508866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.052739022758048516,\n        \"min\": 0.2401448513498028,\n        \"max\": 0.7053942567568356,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5742706573655368,\n          0.5713226769128852,\n          0.5750422337272011\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.052739022758048516,\n        \"min\": 0.2401448513498028,\n        \"max\": 0.7053942567568356,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5742706573655368,\n          0.5713226769128852,\n          0.5750422337272011\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06253864931607932,\n        \"min\": 0.1605184020982094,\n        \"max\": 0.9271791592003475,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5542920796298094,\n          0.5505745867100753,\n          0.5562048088147543\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07309606087514967,\n        \"min\": 0.1754370526155137,\n        \"max\": 0.9514000389371644,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.6191453844328436,\n          0.614226593566705,\n          0.6234744331020378\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06578094324366816,\n        \"min\": 0.1169791453379788,\n        \"max\": 0.7463755873036746,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.5207098421936066,\n          0.5150768386787045,\n          0.5180681584614907\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06909573547194954,\n        \"min\": 0.1983735073844832,\n        \"max\": 0.8713606623616184,\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          0.613519578579038,\n          0.6073934551734517,\n          0.5990389669237263\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = train_df.iloc[8000:12000]\n",
        "#df3.loc[ df3['Label'] == 2.0, 'Label'] = 1.0\n",
        "#df2"
      ],
      "metadata": {
        "id": "g21boO-mIcdF",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3XC3dhueEjoZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df3 = train_df.iloc[12000:16000]\n",
        "#df3.loc[ df3['Label'] == 2.0, 'Label'] = 1.0\n",
        "#df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_pC2lBo3EnOn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# df4 = train_df.iloc[20000:25000]\n",
        "# #df4.loc[ df4['Label'] == 2.0, 'Label'] = 1.0\n",
        "# df4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nEGd9yBVEqGm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "f2e591c1-0da5-4b1e-d894-1a262c37ef26",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0_pPeak   0_tPeak   0_rPeak   0_sPeak   0_qPeak  0_qrs_morph0  \\\n",
              "0      0.438303  0.266989  0.576403  0.408554  0.542624      0.542624   \n",
              "1      0.491285  0.258393  0.535019  0.388107  0.550598      0.550598   \n",
              "2      0.440104  0.391836  0.559018  0.317185  0.545932      0.545932   \n",
              "3      0.486867  0.464057  0.670241  0.285911  0.573835      0.573835   \n",
              "4      0.474577  0.342478  0.424945  0.412417  0.565465      0.565465   \n",
              "...         ...       ...       ...       ...       ...           ...   \n",
              "15995  0.483303  0.550291  0.581599  0.650026  0.556374      0.556374   \n",
              "15996  0.465322  0.283926  0.458206  0.373100  0.530459      0.530459   \n",
              "15997  0.430584  0.404814  0.443528  0.534536  0.536754      0.536754   \n",
              "15998  0.436530  0.308155  0.457760  0.366139  0.545633      0.545633   \n",
              "15999  0.446839  0.407782  0.445832  0.536463  0.555882      0.555882   \n",
              "\n",
              "       0_qrs_morph1  0_qrs_morph3  0_qrs_morph4   1_pPeak   1_tPeak   1_rPeak  \\\n",
              "0          0.488489      0.675400      0.539111  0.292618  0.263815  0.490166   \n",
              "1          0.474882      0.626789      0.615517  0.287513  0.285442  0.478941   \n",
              "2          0.530691      0.635959      0.477287  0.304028  0.325511  0.388111   \n",
              "3          0.601868      0.653305      0.354840  0.302150  0.286775  0.453499   \n",
              "4          0.493817      0.532375      0.540755  0.300088  0.313881  0.518940   \n",
              "...             ...           ...           ...       ...       ...       ...   \n",
              "15995      0.478610      0.498228      0.660334  0.290086  0.257262  0.485063   \n",
              "15996      0.455019      0.515680      0.625787  0.291595  0.333436  0.372481   \n",
              "15997      0.457558      0.486689      0.578516  0.291787  0.280896  0.486842   \n",
              "15998      0.471669      0.562781      0.583974  0.291027  0.323015  0.391966   \n",
              "15999      0.477284      0.508599      0.600098  0.290565  0.281637  0.489241   \n",
              "\n",
              "        1_sPeak   1_qPeak  1_qrs_morph0  1_qrs_morph1  1_qrs_morph2  \\\n",
              "0      0.644686  0.577750      0.577750      0.557462      0.620977   \n",
              "1      0.642074  0.568453      0.568453      0.547180      0.610081   \n",
              "2      0.545082  0.493227      0.493227      0.458583      0.496309   \n",
              "3      0.592144  0.547382      0.547382      0.510903      0.559054   \n",
              "4      0.675353  0.568931      0.568931      0.551942      0.618632   \n",
              "...         ...       ...           ...           ...           ...   \n",
              "15995  0.643471  0.573523      0.573523      0.552084      0.614336   \n",
              "15996  0.553502  0.480282      0.480282      0.443337      0.491740   \n",
              "15997  0.634691  0.574997      0.574997      0.554233      0.615330   \n",
              "15998  0.569713  0.490389      0.490389      0.455241      0.505306   \n",
              "15999  0.631582  0.576007      0.576007      0.556738      0.621373   \n",
              "\n",
              "       1_qrs_morph3  1_qrs_morph4  Label  \n",
              "0          0.519430      0.607794      0  \n",
              "1          0.510764      0.601864      0  \n",
              "2          0.399050      0.478359      0  \n",
              "3          0.457303      0.539486      1  \n",
              "4          0.523256      0.630159      1  \n",
              "...             ...           ...    ...  \n",
              "15995      0.513517      0.603958      0  \n",
              "15996      0.403548      0.488325      0  \n",
              "15997      0.510939      0.596468      0  \n",
              "15998      0.417855      0.503476      0  \n",
              "15999      0.517191      0.598174      0  \n",
              "\n",
              "[16000 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0dbf943-d6d6-494b-87d8-61f4c327195e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_pPeak</th>\n",
              "      <th>0_tPeak</th>\n",
              "      <th>0_rPeak</th>\n",
              "      <th>0_sPeak</th>\n",
              "      <th>0_qPeak</th>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <th>1_pPeak</th>\n",
              "      <th>1_tPeak</th>\n",
              "      <th>1_rPeak</th>\n",
              "      <th>1_sPeak</th>\n",
              "      <th>1_qPeak</th>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.438303</td>\n",
              "      <td>0.266989</td>\n",
              "      <td>0.576403</td>\n",
              "      <td>0.408554</td>\n",
              "      <td>0.542624</td>\n",
              "      <td>0.542624</td>\n",
              "      <td>0.488489</td>\n",
              "      <td>0.675400</td>\n",
              "      <td>0.539111</td>\n",
              "      <td>0.292618</td>\n",
              "      <td>0.263815</td>\n",
              "      <td>0.490166</td>\n",
              "      <td>0.644686</td>\n",
              "      <td>0.577750</td>\n",
              "      <td>0.577750</td>\n",
              "      <td>0.557462</td>\n",
              "      <td>0.620977</td>\n",
              "      <td>0.519430</td>\n",
              "      <td>0.607794</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.491285</td>\n",
              "      <td>0.258393</td>\n",
              "      <td>0.535019</td>\n",
              "      <td>0.388107</td>\n",
              "      <td>0.550598</td>\n",
              "      <td>0.550598</td>\n",
              "      <td>0.474882</td>\n",
              "      <td>0.626789</td>\n",
              "      <td>0.615517</td>\n",
              "      <td>0.287513</td>\n",
              "      <td>0.285442</td>\n",
              "      <td>0.478941</td>\n",
              "      <td>0.642074</td>\n",
              "      <td>0.568453</td>\n",
              "      <td>0.568453</td>\n",
              "      <td>0.547180</td>\n",
              "      <td>0.610081</td>\n",
              "      <td>0.510764</td>\n",
              "      <td>0.601864</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.440104</td>\n",
              "      <td>0.391836</td>\n",
              "      <td>0.559018</td>\n",
              "      <td>0.317185</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.530691</td>\n",
              "      <td>0.635959</td>\n",
              "      <td>0.477287</td>\n",
              "      <td>0.304028</td>\n",
              "      <td>0.325511</td>\n",
              "      <td>0.388111</td>\n",
              "      <td>0.545082</td>\n",
              "      <td>0.493227</td>\n",
              "      <td>0.493227</td>\n",
              "      <td>0.458583</td>\n",
              "      <td>0.496309</td>\n",
              "      <td>0.399050</td>\n",
              "      <td>0.478359</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.486867</td>\n",
              "      <td>0.464057</td>\n",
              "      <td>0.670241</td>\n",
              "      <td>0.285911</td>\n",
              "      <td>0.573835</td>\n",
              "      <td>0.573835</td>\n",
              "      <td>0.601868</td>\n",
              "      <td>0.653305</td>\n",
              "      <td>0.354840</td>\n",
              "      <td>0.302150</td>\n",
              "      <td>0.286775</td>\n",
              "      <td>0.453499</td>\n",
              "      <td>0.592144</td>\n",
              "      <td>0.547382</td>\n",
              "      <td>0.547382</td>\n",
              "      <td>0.510903</td>\n",
              "      <td>0.559054</td>\n",
              "      <td>0.457303</td>\n",
              "      <td>0.539486</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.474577</td>\n",
              "      <td>0.342478</td>\n",
              "      <td>0.424945</td>\n",
              "      <td>0.412417</td>\n",
              "      <td>0.565465</td>\n",
              "      <td>0.565465</td>\n",
              "      <td>0.493817</td>\n",
              "      <td>0.532375</td>\n",
              "      <td>0.540755</td>\n",
              "      <td>0.300088</td>\n",
              "      <td>0.313881</td>\n",
              "      <td>0.518940</td>\n",
              "      <td>0.675353</td>\n",
              "      <td>0.568931</td>\n",
              "      <td>0.568931</td>\n",
              "      <td>0.551942</td>\n",
              "      <td>0.618632</td>\n",
              "      <td>0.523256</td>\n",
              "      <td>0.630159</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15995</th>\n",
              "      <td>0.483303</td>\n",
              "      <td>0.550291</td>\n",
              "      <td>0.581599</td>\n",
              "      <td>0.650026</td>\n",
              "      <td>0.556374</td>\n",
              "      <td>0.556374</td>\n",
              "      <td>0.478610</td>\n",
              "      <td>0.498228</td>\n",
              "      <td>0.660334</td>\n",
              "      <td>0.290086</td>\n",
              "      <td>0.257262</td>\n",
              "      <td>0.485063</td>\n",
              "      <td>0.643471</td>\n",
              "      <td>0.573523</td>\n",
              "      <td>0.573523</td>\n",
              "      <td>0.552084</td>\n",
              "      <td>0.614336</td>\n",
              "      <td>0.513517</td>\n",
              "      <td>0.603958</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>0.465322</td>\n",
              "      <td>0.283926</td>\n",
              "      <td>0.458206</td>\n",
              "      <td>0.373100</td>\n",
              "      <td>0.530459</td>\n",
              "      <td>0.530459</td>\n",
              "      <td>0.455019</td>\n",
              "      <td>0.515680</td>\n",
              "      <td>0.625787</td>\n",
              "      <td>0.291595</td>\n",
              "      <td>0.333436</td>\n",
              "      <td>0.372481</td>\n",
              "      <td>0.553502</td>\n",
              "      <td>0.480282</td>\n",
              "      <td>0.480282</td>\n",
              "      <td>0.443337</td>\n",
              "      <td>0.491740</td>\n",
              "      <td>0.403548</td>\n",
              "      <td>0.488325</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15997</th>\n",
              "      <td>0.430584</td>\n",
              "      <td>0.404814</td>\n",
              "      <td>0.443528</td>\n",
              "      <td>0.534536</td>\n",
              "      <td>0.536754</td>\n",
              "      <td>0.536754</td>\n",
              "      <td>0.457558</td>\n",
              "      <td>0.486689</td>\n",
              "      <td>0.578516</td>\n",
              "      <td>0.291787</td>\n",
              "      <td>0.280896</td>\n",
              "      <td>0.486842</td>\n",
              "      <td>0.634691</td>\n",
              "      <td>0.574997</td>\n",
              "      <td>0.574997</td>\n",
              "      <td>0.554233</td>\n",
              "      <td>0.615330</td>\n",
              "      <td>0.510939</td>\n",
              "      <td>0.596468</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15998</th>\n",
              "      <td>0.436530</td>\n",
              "      <td>0.308155</td>\n",
              "      <td>0.457760</td>\n",
              "      <td>0.366139</td>\n",
              "      <td>0.545633</td>\n",
              "      <td>0.545633</td>\n",
              "      <td>0.471669</td>\n",
              "      <td>0.562781</td>\n",
              "      <td>0.583974</td>\n",
              "      <td>0.291027</td>\n",
              "      <td>0.323015</td>\n",
              "      <td>0.391966</td>\n",
              "      <td>0.569713</td>\n",
              "      <td>0.490389</td>\n",
              "      <td>0.490389</td>\n",
              "      <td>0.455241</td>\n",
              "      <td>0.505306</td>\n",
              "      <td>0.417855</td>\n",
              "      <td>0.503476</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15999</th>\n",
              "      <td>0.446839</td>\n",
              "      <td>0.407782</td>\n",
              "      <td>0.445832</td>\n",
              "      <td>0.536463</td>\n",
              "      <td>0.555882</td>\n",
              "      <td>0.555882</td>\n",
              "      <td>0.477284</td>\n",
              "      <td>0.508599</td>\n",
              "      <td>0.600098</td>\n",
              "      <td>0.290565</td>\n",
              "      <td>0.281637</td>\n",
              "      <td>0.489241</td>\n",
              "      <td>0.631582</td>\n",
              "      <td>0.576007</td>\n",
              "      <td>0.576007</td>\n",
              "      <td>0.556738</td>\n",
              "      <td>0.621373</td>\n",
              "      <td>0.517191</td>\n",
              "      <td>0.598174</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16000 rows  20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0dbf943-d6d6-494b-87d8-61f4c327195e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f0dbf943-d6d6-494b-87d8-61f4c327195e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f0dbf943-d6d6-494b-87d8-61f4c327195e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55f8ffe7-ff17-4c8e-99c2-bbb2e9ae7629\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55f8ffe7-ff17-4c8e-99c2-bbb2e9ae7629')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55f8ffe7-ff17-4c8e-99c2-bbb2e9ae7629 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3ead3925-36ab-4a09-a23a-5a6ad79b72f2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_final')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3ead3925-36ab-4a09-a23a-5a6ad79b72f2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_final');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final",
              "summary": "{\n  \"name\": \"df_final\",\n  \"rows\": 16000,\n  \"fields\": [\n    {\n      \"column\": \"0_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03200056910122813,\n        \"min\": 0.3075823877640271,\n        \"max\": 0.76215691549589,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4573004068722703,\n          0.4406361909151436,\n          0.4771633753572261\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09085167747353573,\n        \"min\": 0.0,\n        \"max\": 0.7457004088200956,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4805922706082204,\n          0.3940809743570945,\n          0.4787767196554129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12298732272031346,\n        \"min\": 0.0,\n        \"max\": 0.7656244430251042,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.205123300483784,\n          0.5654224877965995,\n          0.1977068654315215\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11474374610924078,\n        \"min\": 0.0668516174092629,\n        \"max\": 0.7557581539140097,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2665647975689121,\n          0.3100165833484078,\n          0.2398109355416356\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.045119901187188435,\n        \"min\": 0.2155508831400355,\n        \"max\": 0.8680787086889623,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4041418515118228,\n          0.5455542947002998,\n          0.3973231592176082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.045119901187188435,\n        \"min\": 0.2155508831400355,\n        \"max\": 0.8680787086889623,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4041418515118228,\n          0.5455542947002998,\n          0.3973231592176082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07850549638004721,\n        \"min\": 0.0,\n        \"max\": 0.8127222848443608,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2493456163109918,\n          0.5456188740886639,\n          0.2325669191341415\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12673336050988634,\n        \"min\": 0.0,\n        \"max\": 0.8392720715100526,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2362750915422091,\n          0.6282029515634792,\n          0.2096951482426089\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1190398410377523,\n        \"min\": 0.0,\n        \"max\": 0.9670248226626524,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2631368276407591,\n          0.4733453709558748,\n          0.2299288608449821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021373399868941073,\n        \"min\": 0.103296503627264,\n        \"max\": 0.946746586859583,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2888487006991255,\n          0.3024370057450008,\n          0.2922316845191572\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04142871817638884,\n        \"min\": 0.0833424589103086,\n        \"max\": 0.5740994621296992,\n        \"num_unique_values\": 15998,\n        \"samples\": [\n          0.2618493321739159,\n          0.269622228303406,\n          0.3366537212510448\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06054978106459086,\n        \"min\": 0.0954939494491432,\n        \"max\": 0.9754363708418734,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5551284501812946,\n          0.3944340786365414,\n          0.4766448345901409\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05703814506637455,\n        \"min\": 0.1955124228708535,\n        \"max\": 0.9046390075837883,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5828038525092257,\n          0.5419345516851752,\n          0.6401642436322763\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05111127255474416,\n        \"min\": 0.1857001124068941,\n        \"max\": 0.9332491193327254,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5671782455682178,\n          0.4984640376282012,\n          0.5649630633227518\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05111127255474416,\n        \"min\": 0.1857001124068941,\n        \"max\": 0.9332491193327254,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5671782455682178,\n          0.4984640376282012,\n          0.5649630633227518\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06069681634266127,\n        \"min\": 0.1216373762031989,\n        \"max\": 0.945921448366162,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5871251607642702,\n          0.4647507292791503,\n          0.5430691601854578\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07137295774971862,\n        \"min\": 0.1567992832353253,\n        \"max\": 0.9522165932974588,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.6843553042837652,\n          0.5013116913797464,\n          0.6054701879063594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06478898973060594,\n        \"min\": 0.1000934094099245,\n        \"max\": 0.7463755873036746,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5487430513064904,\n          0.4011731392261828,\n          0.5069861382627775\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06879711614864603,\n        \"min\": 0.1777647696134554,\n        \"max\": 0.8713606623616184,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5939617645894224,\n          0.477592058512391,\n          0.5985508152847284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_final = pd.concat([df0,df1, df2,df3])#df6,df7,df8,df9,df10])\n",
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ewcOrHRfFV0k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "5915cb0a-8837-44bb-ad25-d4867882dbe6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "0    10000\n",
              "1     6000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df_final['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iVL0Bz2gFV9V"
      },
      "outputs": [],
      "source": [
        "#First get the labels and then drop it. Dont run it first.\n",
        "\n",
        "df0_arr = df0.drop(columns=['Label'])\n",
        "df1_arr = df1.drop(columns=['Label'])\n",
        "df2_arr = df2.drop(columns=['Label'])\n",
        "df3_arr = df3.drop(columns=['Label'])\n",
        "#df4_arr = df4.drop(columns=['Label'])\n",
        "#df5_arr = df5.drop(columns=['Label'])\n",
        "# df6_arr = df6.drop(columns=['Label'])\n",
        "# df7_arr = df7.drop(columns=['Label'])\n",
        "# df8_arr = df8.drop(columns=['Label'])\n",
        "# df9_arr = df9.drop(columns=['Label'])\n",
        "# df10_arr = df10.drop(columns=['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Gri5gXocFsJS"
      },
      "outputs": [],
      "source": [
        "df0_arr = df0_arr.to_numpy()\n",
        "df1_arr = df1_arr.to_numpy()\n",
        "df2_arr = df2_arr.to_numpy()\n",
        "df3_arr = df3_arr.to_numpy()\n",
        "#df4_arr = df4_arr.to_numpy()\n",
        "#df5_arr = df5_arr.to_numpy()\n",
        "# df6_arr = df6_arr.to_numpy()\n",
        "# df7_arr = df7_arr.to_numpy()\n",
        "# df8_arr = df8_arr.to_numpy()\n",
        "# df9_arr = df9_arr.to_numpy()\n",
        "# df10_arr = df10_arr.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0gVhhe6fFvKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba63056-9b0e-4724-d730-582c65d9783e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4000, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df0_arr = df0_arr.reshape(1,df0_arr.shape[0],df0_arr.shape[1])\n",
        "df0_arr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "drx-HZKNFyuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d897cbe-18fb-4d47-82b8-7e99ede50cbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4000, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df1_arr = df1_arr.reshape(1,df1_arr.shape[0],df1_arr.shape[1])\n",
        "df1_arr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lUj3yI6zFyyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4396e93-7d8d-4965-a742-4a1f81299c66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4000, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df2_arr = df2_arr.reshape(1,df2_arr.shape[0],df2_arr.shape[1])\n",
        "df2_arr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eYsa21EzFy0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6da5cf3-7635-4433-ae97-80723d5a2d2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4000, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df3_arr = df3_arr.reshape(1,df3_arr.shape[0],df3_arr.shape[1])\n",
        "df3_arr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lRUIqkgVFy3h"
      },
      "outputs": [],
      "source": [
        "# df4_arr = df4_arr.reshape(1,df4_arr.shape[0],df4_arr.shape[1])\n",
        "# df4_arr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EEN5UuMjGEBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932d7104-f6c9-4f94-e866-c08dfdf558e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4000, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "data_final_train = np.vstack((df0_arr,df1_arr,df2_arr,df3_arr))#df6_arr,df7_arr,df8_arr,df9_arr,df10_arr))\n",
        "data_final_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IWPPq_DiGI8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "59de23bf-b82d-400a-ae6c-ac2983a22220",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3        1\n",
              "4        1\n",
              "        ..\n",
              "15995    0\n",
              "15996    0\n",
              "15997    0\n",
              "15998    0\n",
              "15999    0\n",
              "Name: Label, Length: 16000, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15995</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15997</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15998</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15999</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16000 rows  1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "labels = pd.concat([df0['Label'],df1['Label'],df2['Label'],df3['Label']])\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yKRnfeIlGMhv"
      },
      "outputs": [],
      "source": [
        "labels = labels.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JbJ2-ej6_O9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BLezlAVqyeTO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "collapsed": true,
        "outputId": "50a140f0-cfe6-4857-957e-be827a2b8bed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0_pPeak   0_tPeak   0_rPeak   0_sPeak   0_qPeak  0_qrs_morph0  \\\n",
              "0      0.438303  0.266989  0.576403  0.408554  0.542624      0.542624   \n",
              "1      0.491285  0.258393  0.535019  0.388107  0.550598      0.550598   \n",
              "2      0.440104  0.391836  0.559018  0.317185  0.545932      0.545932   \n",
              "3      0.486867  0.464057  0.670241  0.285911  0.573835      0.573835   \n",
              "4      0.474577  0.342478  0.424945  0.412417  0.565465      0.565465   \n",
              "...         ...       ...       ...       ...       ...           ...   \n",
              "15995  0.483303  0.550291  0.581599  0.650026  0.556374      0.556374   \n",
              "15996  0.465322  0.283926  0.458206  0.373100  0.530459      0.530459   \n",
              "15997  0.430584  0.404814  0.443528  0.534536  0.536754      0.536754   \n",
              "15998  0.436530  0.308155  0.457760  0.366139  0.545633      0.545633   \n",
              "15999  0.446839  0.407782  0.445832  0.536463  0.555882      0.555882   \n",
              "\n",
              "       0_qrs_morph1  0_qrs_morph3  0_qrs_morph4   1_pPeak   1_tPeak   1_rPeak  \\\n",
              "0          0.488489      0.675400      0.539111  0.292618  0.263815  0.490166   \n",
              "1          0.474882      0.626789      0.615517  0.287513  0.285442  0.478941   \n",
              "2          0.530691      0.635959      0.477287  0.304028  0.325511  0.388111   \n",
              "3          0.601868      0.653305      0.354840  0.302150  0.286775  0.453499   \n",
              "4          0.493817      0.532375      0.540755  0.300088  0.313881  0.518940   \n",
              "...             ...           ...           ...       ...       ...       ...   \n",
              "15995      0.478610      0.498228      0.660334  0.290086  0.257262  0.485063   \n",
              "15996      0.455019      0.515680      0.625787  0.291595  0.333436  0.372481   \n",
              "15997      0.457558      0.486689      0.578516  0.291787  0.280896  0.486842   \n",
              "15998      0.471669      0.562781      0.583974  0.291027  0.323015  0.391966   \n",
              "15999      0.477284      0.508599      0.600098  0.290565  0.281637  0.489241   \n",
              "\n",
              "        1_sPeak   1_qPeak  1_qrs_morph0  1_qrs_morph1  1_qrs_morph2  \\\n",
              "0      0.644686  0.577750      0.577750      0.557462      0.620977   \n",
              "1      0.642074  0.568453      0.568453      0.547180      0.610081   \n",
              "2      0.545082  0.493227      0.493227      0.458583      0.496309   \n",
              "3      0.592144  0.547382      0.547382      0.510903      0.559054   \n",
              "4      0.675353  0.568931      0.568931      0.551942      0.618632   \n",
              "...         ...       ...           ...           ...           ...   \n",
              "15995  0.643471  0.573523      0.573523      0.552084      0.614336   \n",
              "15996  0.553502  0.480282      0.480282      0.443337      0.491740   \n",
              "15997  0.634691  0.574997      0.574997      0.554233      0.615330   \n",
              "15998  0.569713  0.490389      0.490389      0.455241      0.505306   \n",
              "15999  0.631582  0.576007      0.576007      0.556738      0.621373   \n",
              "\n",
              "       1_qrs_morph3  1_qrs_morph4  Label  \n",
              "0          0.519430      0.607794      0  \n",
              "1          0.510764      0.601864      0  \n",
              "2          0.399050      0.478359      0  \n",
              "3          0.457303      0.539486      1  \n",
              "4          0.523256      0.630159      1  \n",
              "...             ...           ...    ...  \n",
              "15995      0.513517      0.603958      0  \n",
              "15996      0.403548      0.488325      0  \n",
              "15997      0.510939      0.596468      0  \n",
              "15998      0.417855      0.503476      0  \n",
              "15999      0.517191      0.598174      0  \n",
              "\n",
              "[16000 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37647d79-9e2a-44da-abeb-234ff4f35162\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_pPeak</th>\n",
              "      <th>0_tPeak</th>\n",
              "      <th>0_rPeak</th>\n",
              "      <th>0_sPeak</th>\n",
              "      <th>0_qPeak</th>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <th>1_pPeak</th>\n",
              "      <th>1_tPeak</th>\n",
              "      <th>1_rPeak</th>\n",
              "      <th>1_sPeak</th>\n",
              "      <th>1_qPeak</th>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.438303</td>\n",
              "      <td>0.266989</td>\n",
              "      <td>0.576403</td>\n",
              "      <td>0.408554</td>\n",
              "      <td>0.542624</td>\n",
              "      <td>0.542624</td>\n",
              "      <td>0.488489</td>\n",
              "      <td>0.675400</td>\n",
              "      <td>0.539111</td>\n",
              "      <td>0.292618</td>\n",
              "      <td>0.263815</td>\n",
              "      <td>0.490166</td>\n",
              "      <td>0.644686</td>\n",
              "      <td>0.577750</td>\n",
              "      <td>0.577750</td>\n",
              "      <td>0.557462</td>\n",
              "      <td>0.620977</td>\n",
              "      <td>0.519430</td>\n",
              "      <td>0.607794</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.491285</td>\n",
              "      <td>0.258393</td>\n",
              "      <td>0.535019</td>\n",
              "      <td>0.388107</td>\n",
              "      <td>0.550598</td>\n",
              "      <td>0.550598</td>\n",
              "      <td>0.474882</td>\n",
              "      <td>0.626789</td>\n",
              "      <td>0.615517</td>\n",
              "      <td>0.287513</td>\n",
              "      <td>0.285442</td>\n",
              "      <td>0.478941</td>\n",
              "      <td>0.642074</td>\n",
              "      <td>0.568453</td>\n",
              "      <td>0.568453</td>\n",
              "      <td>0.547180</td>\n",
              "      <td>0.610081</td>\n",
              "      <td>0.510764</td>\n",
              "      <td>0.601864</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.440104</td>\n",
              "      <td>0.391836</td>\n",
              "      <td>0.559018</td>\n",
              "      <td>0.317185</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.545932</td>\n",
              "      <td>0.530691</td>\n",
              "      <td>0.635959</td>\n",
              "      <td>0.477287</td>\n",
              "      <td>0.304028</td>\n",
              "      <td>0.325511</td>\n",
              "      <td>0.388111</td>\n",
              "      <td>0.545082</td>\n",
              "      <td>0.493227</td>\n",
              "      <td>0.493227</td>\n",
              "      <td>0.458583</td>\n",
              "      <td>0.496309</td>\n",
              "      <td>0.399050</td>\n",
              "      <td>0.478359</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.486867</td>\n",
              "      <td>0.464057</td>\n",
              "      <td>0.670241</td>\n",
              "      <td>0.285911</td>\n",
              "      <td>0.573835</td>\n",
              "      <td>0.573835</td>\n",
              "      <td>0.601868</td>\n",
              "      <td>0.653305</td>\n",
              "      <td>0.354840</td>\n",
              "      <td>0.302150</td>\n",
              "      <td>0.286775</td>\n",
              "      <td>0.453499</td>\n",
              "      <td>0.592144</td>\n",
              "      <td>0.547382</td>\n",
              "      <td>0.547382</td>\n",
              "      <td>0.510903</td>\n",
              "      <td>0.559054</td>\n",
              "      <td>0.457303</td>\n",
              "      <td>0.539486</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.474577</td>\n",
              "      <td>0.342478</td>\n",
              "      <td>0.424945</td>\n",
              "      <td>0.412417</td>\n",
              "      <td>0.565465</td>\n",
              "      <td>0.565465</td>\n",
              "      <td>0.493817</td>\n",
              "      <td>0.532375</td>\n",
              "      <td>0.540755</td>\n",
              "      <td>0.300088</td>\n",
              "      <td>0.313881</td>\n",
              "      <td>0.518940</td>\n",
              "      <td>0.675353</td>\n",
              "      <td>0.568931</td>\n",
              "      <td>0.568931</td>\n",
              "      <td>0.551942</td>\n",
              "      <td>0.618632</td>\n",
              "      <td>0.523256</td>\n",
              "      <td>0.630159</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15995</th>\n",
              "      <td>0.483303</td>\n",
              "      <td>0.550291</td>\n",
              "      <td>0.581599</td>\n",
              "      <td>0.650026</td>\n",
              "      <td>0.556374</td>\n",
              "      <td>0.556374</td>\n",
              "      <td>0.478610</td>\n",
              "      <td>0.498228</td>\n",
              "      <td>0.660334</td>\n",
              "      <td>0.290086</td>\n",
              "      <td>0.257262</td>\n",
              "      <td>0.485063</td>\n",
              "      <td>0.643471</td>\n",
              "      <td>0.573523</td>\n",
              "      <td>0.573523</td>\n",
              "      <td>0.552084</td>\n",
              "      <td>0.614336</td>\n",
              "      <td>0.513517</td>\n",
              "      <td>0.603958</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>0.465322</td>\n",
              "      <td>0.283926</td>\n",
              "      <td>0.458206</td>\n",
              "      <td>0.373100</td>\n",
              "      <td>0.530459</td>\n",
              "      <td>0.530459</td>\n",
              "      <td>0.455019</td>\n",
              "      <td>0.515680</td>\n",
              "      <td>0.625787</td>\n",
              "      <td>0.291595</td>\n",
              "      <td>0.333436</td>\n",
              "      <td>0.372481</td>\n",
              "      <td>0.553502</td>\n",
              "      <td>0.480282</td>\n",
              "      <td>0.480282</td>\n",
              "      <td>0.443337</td>\n",
              "      <td>0.491740</td>\n",
              "      <td>0.403548</td>\n",
              "      <td>0.488325</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15997</th>\n",
              "      <td>0.430584</td>\n",
              "      <td>0.404814</td>\n",
              "      <td>0.443528</td>\n",
              "      <td>0.534536</td>\n",
              "      <td>0.536754</td>\n",
              "      <td>0.536754</td>\n",
              "      <td>0.457558</td>\n",
              "      <td>0.486689</td>\n",
              "      <td>0.578516</td>\n",
              "      <td>0.291787</td>\n",
              "      <td>0.280896</td>\n",
              "      <td>0.486842</td>\n",
              "      <td>0.634691</td>\n",
              "      <td>0.574997</td>\n",
              "      <td>0.574997</td>\n",
              "      <td>0.554233</td>\n",
              "      <td>0.615330</td>\n",
              "      <td>0.510939</td>\n",
              "      <td>0.596468</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15998</th>\n",
              "      <td>0.436530</td>\n",
              "      <td>0.308155</td>\n",
              "      <td>0.457760</td>\n",
              "      <td>0.366139</td>\n",
              "      <td>0.545633</td>\n",
              "      <td>0.545633</td>\n",
              "      <td>0.471669</td>\n",
              "      <td>0.562781</td>\n",
              "      <td>0.583974</td>\n",
              "      <td>0.291027</td>\n",
              "      <td>0.323015</td>\n",
              "      <td>0.391966</td>\n",
              "      <td>0.569713</td>\n",
              "      <td>0.490389</td>\n",
              "      <td>0.490389</td>\n",
              "      <td>0.455241</td>\n",
              "      <td>0.505306</td>\n",
              "      <td>0.417855</td>\n",
              "      <td>0.503476</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15999</th>\n",
              "      <td>0.446839</td>\n",
              "      <td>0.407782</td>\n",
              "      <td>0.445832</td>\n",
              "      <td>0.536463</td>\n",
              "      <td>0.555882</td>\n",
              "      <td>0.555882</td>\n",
              "      <td>0.477284</td>\n",
              "      <td>0.508599</td>\n",
              "      <td>0.600098</td>\n",
              "      <td>0.290565</td>\n",
              "      <td>0.281637</td>\n",
              "      <td>0.489241</td>\n",
              "      <td>0.631582</td>\n",
              "      <td>0.576007</td>\n",
              "      <td>0.576007</td>\n",
              "      <td>0.556738</td>\n",
              "      <td>0.621373</td>\n",
              "      <td>0.517191</td>\n",
              "      <td>0.598174</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16000 rows  20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37647d79-9e2a-44da-abeb-234ff4f35162')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-37647d79-9e2a-44da-abeb-234ff4f35162 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-37647d79-9e2a-44da-abeb-234ff4f35162');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-31bde84f-2a2f-4f3b-84e7-29eca1a119ac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31bde84f-2a2f-4f3b-84e7-29eca1a119ac')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-31bde84f-2a2f-4f3b-84e7-29eca1a119ac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_943aa0de-4609-44df-b814-92e9c9658ed2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('VEB_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_943aa0de-4609-44df-b814-92e9c9658ed2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('VEB_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "VEB_test",
              "summary": "{\n  \"name\": \"VEB_test\",\n  \"rows\": 16000,\n  \"fields\": [\n    {\n      \"column\": \"0_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03200056910122813,\n        \"min\": 0.3075823877640271,\n        \"max\": 0.76215691549589,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4573004068722703,\n          0.4406361909151436,\n          0.4771633753572261\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09085167747353573,\n        \"min\": 0.0,\n        \"max\": 0.7457004088200956,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4805922706082204,\n          0.3940809743570945,\n          0.4787767196554129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12298732272031346,\n        \"min\": 0.0,\n        \"max\": 0.7656244430251042,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.205123300483784,\n          0.5654224877965995,\n          0.1977068654315215\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11474374610924078,\n        \"min\": 0.0668516174092629,\n        \"max\": 0.7557581539140097,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2665647975689121,\n          0.3100165833484078,\n          0.2398109355416356\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.045119901187188435,\n        \"min\": 0.2155508831400355,\n        \"max\": 0.8680787086889623,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4041418515118228,\n          0.5455542947002998,\n          0.3973231592176082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.045119901187188435,\n        \"min\": 0.2155508831400355,\n        \"max\": 0.8680787086889623,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.4041418515118228,\n          0.5455542947002998,\n          0.3973231592176082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07850549638004721,\n        \"min\": 0.0,\n        \"max\": 0.8127222848443608,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2493456163109918,\n          0.5456188740886639,\n          0.2325669191341415\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12673336050988634,\n        \"min\": 0.0,\n        \"max\": 0.8392720715100526,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2362750915422091,\n          0.6282029515634792,\n          0.2096951482426089\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1190398410377523,\n        \"min\": 0.0,\n        \"max\": 0.9670248226626524,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2631368276407591,\n          0.4733453709558748,\n          0.2299288608449821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021373399868941073,\n        \"min\": 0.103296503627264,\n        \"max\": 0.946746586859583,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.2888487006991255,\n          0.3024370057450008,\n          0.2922316845191572\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04142871817638884,\n        \"min\": 0.0833424589103086,\n        \"max\": 0.5740994621296992,\n        \"num_unique_values\": 15998,\n        \"samples\": [\n          0.2618493321739159,\n          0.269622228303406,\n          0.3366537212510448\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06054978106459086,\n        \"min\": 0.0954939494491432,\n        \"max\": 0.9754363708418734,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5551284501812946,\n          0.3944340786365414,\n          0.4766448345901409\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05703814506637455,\n        \"min\": 0.1955124228708535,\n        \"max\": 0.9046390075837883,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5828038525092257,\n          0.5419345516851752,\n          0.6401642436322763\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05111127255474416,\n        \"min\": 0.1857001124068941,\n        \"max\": 0.9332491193327254,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5671782455682178,\n          0.4984640376282012,\n          0.5649630633227518\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05111127255474416,\n        \"min\": 0.1857001124068941,\n        \"max\": 0.9332491193327254,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5671782455682178,\n          0.4984640376282012,\n          0.5649630633227518\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06069681634266127,\n        \"min\": 0.1216373762031989,\n        \"max\": 0.945921448366162,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5871251607642702,\n          0.4647507292791503,\n          0.5430691601854578\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07137295774971862,\n        \"min\": 0.1567992832353253,\n        \"max\": 0.9522165932974588,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.6843553042837652,\n          0.5013116913797464,\n          0.6054701879063594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06478898973060594,\n        \"min\": 0.1000934094099245,\n        \"max\": 0.7463755873036746,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5487430513064904,\n          0.4011731392261828,\n          0.5069861382627775\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06879711614864603,\n        \"min\": 0.1777647696134554,\n        \"max\": 0.8713606623616184,\n        \"num_unique_values\": 16000,\n        \"samples\": [\n          0.5939617645894224,\n          0.477592058512391,\n          0.5985508152847284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#BOT_test = pd.read_csv('/content/gdrive/Shareddrives/Anomaly/dataset/BOT.csv')\n",
        "\n",
        "VEB_test = pd.read_csv('TRAIN_DATA_NORM_VEB_16K.csv')\n",
        "VEB_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BOT_test.isna().sum()"
      ],
      "metadata": {
        "id": "Hw0FtmD5UEBB",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tdSnI5zVy14g"
      },
      "outputs": [],
      "source": [
        "#X_BOT_test = BOT_test.drop(labels = ['Label'], axis=1)\n",
        "VEB_test_x = VEB_test.drop(labels = ['Label'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gwaHv5N2y2Dp"
      },
      "outputs": [],
      "source": [
        "#X_BOT_test = X_BOT_test.to_numpy()\n",
        "X_VEB_test = VEB_test_x.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IK_IqdOhy2G9"
      },
      "outputs": [],
      "source": [
        "#y_BOT_test = BOT_test['Label']\n",
        "y_VEB_test = VEB_test['Label']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_BOT_test.value_counts()\n",
        "\n",
        "y_VEB_test.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "GV9ZvhnLJMt5",
        "outputId": "3cabd6b1-0516-41cb-e575-4c9d0e02441a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "0    10000\n",
              "1     6000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "neCGc3bMKT8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d37f86-0725-4f82-88bf-d40d176e5081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-96dcf12c1c85>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y_VEB_test[y_VEB_test>1.0] = 1.0\n"
          ]
        }
      ],
      "source": [
        "#y_BOT_test[y_BOT_test>1.0] = 1.0\n",
        "\n",
        "y_VEB_test[y_VEB_test>1.0] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "34Iy_GYgzW-K"
      },
      "outputs": [],
      "source": [
        "#y_BOT_test = y_BOT_test.to_numpy()\n",
        "\n",
        "y_VEB_test = y_VEB_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "501k9OavNqVl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RARE_test = pd.read_csv('/content/gdrive/Shareddrives/Anomaly/dataset/RARE.csv')\n",
        "\n",
        "F_test = pd.read_csv('ARRYTH_F.csv')"
      ],
      "metadata": {
        "id": "ux2AHkSX6iO8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RARE_test.isna().sum()\n",
        "F_test.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "O2dcUmpaUA2K",
        "outputId": "bbc9618c-b5a7-42b4-cae5-579e5e42ec4f",
        "collapsed": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0_pPeak         0\n",
              "0_tPeak         0\n",
              "0_rPeak         0\n",
              "0_sPeak         0\n",
              "0_qPeak         0\n",
              "0_qrs_morph0    0\n",
              "0_qrs_morph1    0\n",
              "0_qrs_morph3    0\n",
              "0_qrs_morph4    0\n",
              "1_pPeak         0\n",
              "1_tPeak         0\n",
              "1_rPeak         0\n",
              "1_sPeak         0\n",
              "1_qPeak         0\n",
              "1_qrs_morph0    0\n",
              "1_qrs_morph1    0\n",
              "1_qrs_morph2    0\n",
              "1_qrs_morph3    0\n",
              "1_qrs_morph4    0\n",
              "Label           0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_RARE_test = RARE_test.drop(labels = ['Label'], axis=1)\n",
        "X_F_test = F_test.drop(labels = ['Label'], axis=1)"
      ],
      "metadata": {
        "id": "aO9rM07J6njp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_RARE_test = X_RARE_test.to_numpy()\n",
        "\n",
        "X_F_test = X_F_test.to_numpy()"
      ],
      "metadata": {
        "id": "r7xaVHob6tqa"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_RARE_test = RARE_test['Label']\n",
        "\n",
        "y_F_test = F_test['Label']"
      ],
      "metadata": {
        "id": "1ng5Yvzv6w7k"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_RARE_test.value_counts()\n",
        "y_F_test.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "gGsEJ6PfJa7x",
        "outputId": "b0daed62-d956-47e3-a1e5-803b337f6ae7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1    803\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_RARE_test[y_RARE_test>1.0] = 1.0\n",
        "\n",
        "y_F_test[y_F_test>1.0] = 1.0"
      ],
      "metadata": {
        "id": "nwMU2Kze6zhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23c49f7-c347-4132-987b-c1f17a0a226c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-89c53389011e>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y_F_test[y_F_test>1.0] = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_RARE_test = y_RARE_test.to_numpy()\n",
        "\n",
        "y_F_test = y_F_test.to_numpy()"
      ],
      "metadata": {
        "id": "H3z9p3Y-621J"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAVrdmCHNrdK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SlowHTTPTest_test = pd.read_csv('/content/gdrive/Shareddrives/Anomaly/dataset/SlowHTTPTest.csv')\n",
        "\n",
        "SVEB_test = pd.read_csv('ARRYTH_SVEB_TEST.csv')"
      ],
      "metadata": {
        "id": "J2EAu1m9Juo_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SlowHTTPTest_test.isna().sum()\n",
        "\n",
        "SVEB_test.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "uUeB9kfWT9YB",
        "outputId": "d299f486-328f-4c7d-ed4d-a3dbde779f44",
        "collapsed": true
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0_pPeak         0\n",
              "0_tPeak         0\n",
              "0_rPeak         0\n",
              "0_sPeak         0\n",
              "0_qPeak         0\n",
              "0_qrs_morph0    0\n",
              "0_qrs_morph1    0\n",
              "0_qrs_morph3    0\n",
              "0_qrs_morph4    0\n",
              "1_pPeak         0\n",
              "1_tPeak         0\n",
              "1_rPeak         0\n",
              "1_sPeak         0\n",
              "1_qPeak         0\n",
              "1_qrs_morph0    0\n",
              "1_qrs_morph1    0\n",
              "1_qrs_morph2    0\n",
              "1_qrs_morph3    0\n",
              "1_qrs_morph4    0\n",
              "Label           0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_SlowHTTPTest_test = SlowHTTPTest_test.drop(labels = ['Label'], axis=1)\n",
        "\n",
        "X_SVEB_test = SVEB_test.drop(labels = ['Label'], axis=1)"
      ],
      "metadata": {
        "id": "e1VfT_jrJ3w8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_SlowHTTPTest_test = X_SlowHTTPTest_test.to_numpy()\n",
        "\n",
        "X_SVEB_test = X_SVEB_test.to_numpy()"
      ],
      "metadata": {
        "id": "ihcdP5TVJ7x7"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_SlowHTTPTest_test = SlowHTTPTest_test['Label']\n",
        "\n",
        "y_SVEB_test = SVEB_test['Label']"
      ],
      "metadata": {
        "id": "pProSogILjlv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_SlowHTTPTest_test.value_counts()\n",
        "\n",
        "y_SVEB_test.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "dxJguzAaJ_T3",
        "outputId": "0ba95e6c-ed52-4eb3-b779-87fba0ab6dbb",
        "collapsed": true
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1    1000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_SlowHTTPTest_test[y_SlowHTTPTest_test>1.0] = 1.0\n",
        "\n",
        "y_SVEB_test[y_SVEB_test>1.0] = 1.0"
      ],
      "metadata": {
        "id": "PIeMnYPRKCtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940e45e7-e35a-4643-82b6-f525e117ea55"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-621b3290515b>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y_SVEB_test[y_SVEB_test>1.0] = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_SlowHTTPTest_test = y_SlowHTTPTest_test.to_numpy()\n",
        "\n",
        "y_SVEB_test = y_SVEB_test.to_numpy()"
      ],
      "metadata": {
        "id": "UgVlAx3gLtXf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ev2B3ULQLyqY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OOD_INFIL_test = pd.read_csv('/content/gdrive/Shareddrives/Anomaly/dataset/OOD_INFIL.csv')\n",
        "\n",
        "VEB_test = pd.read_csv('ARRYTH_VEB.csv')"
      ],
      "metadata": {
        "id": "hOvRT2h7MN9I"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OOD_INFIL_test.isna().sum()\n",
        "\n",
        "VEB_test.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "8g_3xNA5T1QC",
        "outputId": "307bdea2-fac9-47bb-84f8-dc3fdcef3718",
        "collapsed": true
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0_pPeak         0\n",
              "0_tPeak         0\n",
              "0_rPeak         0\n",
              "0_sPeak         0\n",
              "0_qPeak         0\n",
              "0_qrs_morph0    0\n",
              "0_qrs_morph1    0\n",
              "0_qrs_morph3    0\n",
              "0_qrs_morph4    0\n",
              "1_pPeak         0\n",
              "1_tPeak         0\n",
              "1_rPeak         0\n",
              "1_sPeak         0\n",
              "1_qPeak         0\n",
              "1_qrs_morph0    0\n",
              "1_qrs_morph1    0\n",
              "1_qrs_morph2    0\n",
              "1_qrs_morph3    0\n",
              "1_qrs_morph4    0\n",
              "Label           0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_OOD_INFIL_test = OOD_INFIL_test.drop(labels = ['Label'], axis=1)\n",
        "\n",
        "X_VEB_test = VEB_test.drop(labels = ['Label'], axis=1)"
      ],
      "metadata": {
        "id": "VEJGZYcDMN9K"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_OOD_INFIL_test = X_OOD_INFIL_test.to_numpy()\n",
        "\n",
        "X_VEB_test = X_VEB_test.to_numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "4lUnRZF5MN9K"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_OOD_INFIL_test = OOD_INFIL_test['Label']\n",
        "\n",
        "y_VEB_test = VEB_test['Label']"
      ],
      "metadata": {
        "id": "Wwyudk68MN9L"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_OOD_INFIL_test.value_counts()\n",
        "\n",
        "y_VEB_test.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "hy_NIfjCMN9L",
        "outputId": "8adf5f4b-127e-44dc-e646-eb8f36126400"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1    7009\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_OOD_INFIL_test[y_OOD_INFIL_test>1.0] = 1.0\n",
        "\n",
        "y_VEB_test[y_VEB_test>1.0] = 1.0"
      ],
      "metadata": {
        "id": "c8OBs6ROMN9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2d4670-fa90-4b69-9544-b520684a9b90"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-50b917ff398d>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y_VEB_test[y_VEB_test>1.0] = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_OOD_INFIL_test = y_OOD_INFIL_test.to_numpy()\n",
        "\n",
        "y_VEB_test = y_VEB_test.to_numpy()"
      ],
      "metadata": {
        "id": "BEQ7SxMaMN9M"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5B8-x4-MQl1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DDOS_SOLARIS_test = pd.read_csv('/content/gdrive/Shareddrives/Anomaly/dataset/DDOS_SOLARIS.csv')\n",
        "\n",
        "Q_test = pd.read_csv('ARRYTH_Q.csv')"
      ],
      "metadata": {
        "id": "9t7sf5wxP5-P"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DDOS_SOLARIS_test.isna().sum()\n",
        "\n",
        "Q_test.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "H4FxqEWpS9r4",
        "outputId": "1a67e831-a43b-4322-fc51-a3dc947e0a0c",
        "collapsed": true
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0_pPeak         0\n",
              "0_tPeak         0\n",
              "0_rPeak         0\n",
              "0_sPeak         0\n",
              "0_qPeak         0\n",
              "0_qrs_morph0    0\n",
              "0_qrs_morph1    0\n",
              "0_qrs_morph3    0\n",
              "0_qrs_morph4    0\n",
              "1_pPeak         0\n",
              "1_tPeak         0\n",
              "1_rPeak         0\n",
              "1_sPeak         0\n",
              "1_qPeak         0\n",
              "1_qrs_morph0    0\n",
              "1_qrs_morph1    0\n",
              "1_qrs_morph2    0\n",
              "1_qrs_morph3    0\n",
              "1_qrs_morph4    0\n",
              "Label           0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_DDOS_SOLARIS_test = DDOS_SOLARIS_test.drop(labels = ['Label'], axis=1)\n",
        "\n",
        "X_Q_test = Q_test.drop(labels = ['Label'], axis=1)"
      ],
      "metadata": {
        "id": "FYum_XZhP5-Q"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_DDOS_SOLARIS_test = X_DDOS_SOLARIS_test.to_numpy()\n",
        "\n",
        "X_Q_test = X_Q_test.to_numpy()"
      ],
      "metadata": {
        "id": "gEdV5E57P5-R"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_SOLARIS_test = DDOS_SOLARIS_test['Label']\n",
        "\n",
        "y_Q_test = Q_test['Label']"
      ],
      "metadata": {
        "id": "lJ5lVbdVP5-R"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_SOLARIS_test.value_counts()\n",
        "\n",
        "y_Q_test.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "Y6CkF03aP5-R",
        "outputId": "8afb4b68-52db-457f-f5f0-d6f1525866c9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1    15\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_SOLARIS_test[y_DDOS_SOLARIS_test>1.0] = 1.0\n",
        "\n",
        "y_Q_test[y_Q_test>1.0] = 1.0"
      ],
      "metadata": {
        "id": "8Ihr044nP5-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a5e235-2c87-486c-d557-81a46c629222"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-6c1bbd67df04>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y_Q_test[y_Q_test>1.0] = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_SOLARIS_test = y_DDOS_SOLARIS_test.to_numpy()\n",
        "\n",
        "y_Q_test = y_Q_test.to_numpy()"
      ],
      "metadata": {
        "id": "a55GgrduP5-R"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5vSQd-dQiJm"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DDOS_HOIC_test = pd.read_csv('/content/gdrive/Shareddrives/Anomaly/dataset/DDOS_HOIC.csv')"
      ],
      "metadata": {
        "id": "OPt0FupfRg6p"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DDOS_HOIC_test.isna().sum()"
      ],
      "metadata": {
        "id": "WR92c5rGS6Zl",
        "collapsed": true
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_DDOS_HOIC_test = DDOS_HOIC_test.drop(labels = ['Label'], axis=1)"
      ],
      "metadata": {
        "id": "hlpwIV7oRg6q"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_DDOS_HOIC_test = X_DDOS_HOIC_test.to_numpy()"
      ],
      "metadata": {
        "id": "Uipksm-oRg6r"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_HOIC_test = DDOS_HOIC_test['Label']"
      ],
      "metadata": {
        "id": "77R4ewAdRg6s"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_HOIC_test.value_counts()"
      ],
      "metadata": {
        "id": "3FrY6hf2Rg6s"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_HOIC_test[y_DDOS_HOIC_test>1.0] = 1.0"
      ],
      "metadata": {
        "id": "6sONLIVBRg6s"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_HOIC_test = y_DDOS_HOIC_test.to_numpy()"
      ],
      "metadata": {
        "id": "f4xqloRQRg6t"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qy7Mh9hJQlj0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DDOS_GOLDEN_EYE_test = pd.read_csv('/content/gdrive/Shareddrives/Anomaly/dataset/DDOS_GOLDEN_EYE.csv')"
      ],
      "metadata": {
        "id": "wEZftlpGSSHP"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DDOS_GOLDEN_EYE_test.isna().sum()"
      ],
      "metadata": {
        "id": "SjSv5pGIS2CA",
        "collapsed": true
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_DDOS_GOLDEN_EYE_test = DDOS_GOLDEN_EYE_test.drop(labels = ['Label'], axis=1)"
      ],
      "metadata": {
        "id": "Ciz0zl7ASSHP"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_DDOS_GOLDEN_EYE_test = X_DDOS_GOLDEN_EYE_test.to_numpy()"
      ],
      "metadata": {
        "id": "HS13KkOpSSHQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_GOLDEN_EYE_test = DDOS_GOLDEN_EYE_test['Label']"
      ],
      "metadata": {
        "id": "3Q6gIeI_SSHQ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_GOLDEN_EYE_test.value_counts()"
      ],
      "metadata": {
        "id": "L-gBqvnnSSHR"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_GOLDEN_EYE_test[y_DDOS_GOLDEN_EYE_test>1.0] = 1.0"
      ],
      "metadata": {
        "id": "jsHb1THdSSHR"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_DDOS_GOLDEN_EYE_test = y_DDOS_GOLDEN_EYE_test.to_numpy()"
      ],
      "metadata": {
        "id": "j28EPvAsSSHS"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szDmPRr1THAN"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BENIGN_TEST_test = pd.read_csv('/content/gdrive/Shareddrives/Anomaly/dataset/BENIGN_TEST.csv')\n",
        "\n",
        "BENIGN_TEST_test = pd.read_csv('ARRYTH_NORM_TEST.csv') #BENIGN_TEST.csv')\n",
        "BENIGN_TEST_test"
      ],
      "metadata": {
        "id": "Eiq6T905TOgk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "collapsed": true,
        "outputId": "443579cf-1ca4-44c5-df65-06e160eca7cf"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0_pPeak   0_tPeak   0_rPeak   0_sPeak   0_qPeak  0_qrs_morph0  \\\n",
              "0     0.443052  0.295496  0.613567  0.368829  0.545137      0.545137   \n",
              "1     0.444765  0.584627  0.611435  0.674982  0.545853      0.545853   \n",
              "2     0.447429  0.568965  0.601506  0.666677  0.548741      0.548741   \n",
              "3     0.445831  0.295385  0.591995  0.374701  0.548165      0.548165   \n",
              "4     0.444439  0.299496  0.609698  0.374006  0.547352      0.547352   \n",
              "...        ...       ...       ...       ...       ...           ...   \n",
              "9995  0.434000  0.589010  0.612437  0.675821  0.543406      0.543406   \n",
              "9996  0.432074  0.587216  0.619711  0.681905  0.541208      0.541208   \n",
              "9997  0.429878  0.282175  0.638090  0.297741  0.538950      0.538950   \n",
              "9998  0.468294  0.602032  0.621016  0.682996  0.529300      0.529300   \n",
              "9999  0.428189  0.285268  0.630830  0.300064  0.537041      0.537041   \n",
              "\n",
              "      0_qrs_morph1  0_qrs_morph3  0_qrs_morph4   1_pPeak   1_tPeak   1_rPeak  \\\n",
              "0         0.495019      0.725160      0.601062  0.300290  0.269418  0.580159   \n",
              "1         0.471386      0.558154      0.705977  0.305670  0.347795  0.587302   \n",
              "2         0.478150      0.567283      0.711144  0.296664  0.332686  0.571533   \n",
              "3         0.498104      0.705593      0.556872  0.298806  0.266127  0.569975   \n",
              "4         0.503817      0.726136      0.612336  0.299477  0.337599  0.578348   \n",
              "...            ...           ...           ...       ...       ...       ...   \n",
              "9995      0.465107      0.486840      0.633324  0.288649  0.255933  0.582924   \n",
              "9996      0.465453      0.483403      0.588433  0.286784  0.260386  0.588277   \n",
              "9997      0.468862      0.518470      0.835544  0.289645  0.257755  0.584616   \n",
              "9998      0.449027      0.469596      0.543538  0.286994  0.253157  0.584602   \n",
              "9999      0.461676      0.492096      0.829782  0.288728  0.257440  0.587936   \n",
              "\n",
              "       1_sPeak   1_qPeak  1_qrs_morph0  1_qrs_morph1  1_qrs_morph2  \\\n",
              "0     0.587926  0.580332      0.580332      0.562380      0.628353   \n",
              "1     0.732228  0.585745      0.585745      0.568408      0.635614   \n",
              "2     0.719109  0.580952      0.580952      0.562668      0.635022   \n",
              "3     0.591316  0.582799      0.582799      0.566780      0.664067   \n",
              "4     0.724779  0.580184      0.580184      0.561636      0.627403   \n",
              "...        ...       ...           ...           ...           ...   \n",
              "9995  0.555552  0.574983      0.574983      0.560987      0.675427   \n",
              "9996  0.543051  0.573261      0.573261      0.556591      0.643054   \n",
              "9997  0.547672  0.574312      0.574312      0.558022      0.628012   \n",
              "9998  0.556309  0.572743      0.572743      0.559772      0.670048   \n",
              "9999  0.549279  0.572244      0.572244      0.554444      0.623063   \n",
              "\n",
              "      1_qrs_morph3  1_qrs_morph4  Label  \n",
              "0         0.549820      0.697921      0  \n",
              "1         0.537340      0.648027      0  \n",
              "2         0.545000      0.670679      0  \n",
              "3         0.602445      0.632089      0  \n",
              "4         0.527490      0.638360      0  \n",
              "...            ...           ...    ...  \n",
              "9995      0.615486      0.626019      0  \n",
              "9996      0.615545      0.636374      0  \n",
              "9997      0.600284      0.670510      0  \n",
              "9998      0.617177      0.640198      0  \n",
              "9999      0.569159      0.696378      0  \n",
              "\n",
              "[10000 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27d17579-55d0-41a0-b403-d5f15b5bbcb8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_pPeak</th>\n",
              "      <th>0_tPeak</th>\n",
              "      <th>0_rPeak</th>\n",
              "      <th>0_sPeak</th>\n",
              "      <th>0_qPeak</th>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <th>1_pPeak</th>\n",
              "      <th>1_tPeak</th>\n",
              "      <th>1_rPeak</th>\n",
              "      <th>1_sPeak</th>\n",
              "      <th>1_qPeak</th>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.443052</td>\n",
              "      <td>0.295496</td>\n",
              "      <td>0.613567</td>\n",
              "      <td>0.368829</td>\n",
              "      <td>0.545137</td>\n",
              "      <td>0.545137</td>\n",
              "      <td>0.495019</td>\n",
              "      <td>0.725160</td>\n",
              "      <td>0.601062</td>\n",
              "      <td>0.300290</td>\n",
              "      <td>0.269418</td>\n",
              "      <td>0.580159</td>\n",
              "      <td>0.587926</td>\n",
              "      <td>0.580332</td>\n",
              "      <td>0.580332</td>\n",
              "      <td>0.562380</td>\n",
              "      <td>0.628353</td>\n",
              "      <td>0.549820</td>\n",
              "      <td>0.697921</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.444765</td>\n",
              "      <td>0.584627</td>\n",
              "      <td>0.611435</td>\n",
              "      <td>0.674982</td>\n",
              "      <td>0.545853</td>\n",
              "      <td>0.545853</td>\n",
              "      <td>0.471386</td>\n",
              "      <td>0.558154</td>\n",
              "      <td>0.705977</td>\n",
              "      <td>0.305670</td>\n",
              "      <td>0.347795</td>\n",
              "      <td>0.587302</td>\n",
              "      <td>0.732228</td>\n",
              "      <td>0.585745</td>\n",
              "      <td>0.585745</td>\n",
              "      <td>0.568408</td>\n",
              "      <td>0.635614</td>\n",
              "      <td>0.537340</td>\n",
              "      <td>0.648027</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.447429</td>\n",
              "      <td>0.568965</td>\n",
              "      <td>0.601506</td>\n",
              "      <td>0.666677</td>\n",
              "      <td>0.548741</td>\n",
              "      <td>0.548741</td>\n",
              "      <td>0.478150</td>\n",
              "      <td>0.567283</td>\n",
              "      <td>0.711144</td>\n",
              "      <td>0.296664</td>\n",
              "      <td>0.332686</td>\n",
              "      <td>0.571533</td>\n",
              "      <td>0.719109</td>\n",
              "      <td>0.580952</td>\n",
              "      <td>0.580952</td>\n",
              "      <td>0.562668</td>\n",
              "      <td>0.635022</td>\n",
              "      <td>0.545000</td>\n",
              "      <td>0.670679</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.445831</td>\n",
              "      <td>0.295385</td>\n",
              "      <td>0.591995</td>\n",
              "      <td>0.374701</td>\n",
              "      <td>0.548165</td>\n",
              "      <td>0.548165</td>\n",
              "      <td>0.498104</td>\n",
              "      <td>0.705593</td>\n",
              "      <td>0.556872</td>\n",
              "      <td>0.298806</td>\n",
              "      <td>0.266127</td>\n",
              "      <td>0.569975</td>\n",
              "      <td>0.591316</td>\n",
              "      <td>0.582799</td>\n",
              "      <td>0.582799</td>\n",
              "      <td>0.566780</td>\n",
              "      <td>0.664067</td>\n",
              "      <td>0.602445</td>\n",
              "      <td>0.632089</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.444439</td>\n",
              "      <td>0.299496</td>\n",
              "      <td>0.609698</td>\n",
              "      <td>0.374006</td>\n",
              "      <td>0.547352</td>\n",
              "      <td>0.547352</td>\n",
              "      <td>0.503817</td>\n",
              "      <td>0.726136</td>\n",
              "      <td>0.612336</td>\n",
              "      <td>0.299477</td>\n",
              "      <td>0.337599</td>\n",
              "      <td>0.578348</td>\n",
              "      <td>0.724779</td>\n",
              "      <td>0.580184</td>\n",
              "      <td>0.580184</td>\n",
              "      <td>0.561636</td>\n",
              "      <td>0.627403</td>\n",
              "      <td>0.527490</td>\n",
              "      <td>0.638360</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>0.434000</td>\n",
              "      <td>0.589010</td>\n",
              "      <td>0.612437</td>\n",
              "      <td>0.675821</td>\n",
              "      <td>0.543406</td>\n",
              "      <td>0.543406</td>\n",
              "      <td>0.465107</td>\n",
              "      <td>0.486840</td>\n",
              "      <td>0.633324</td>\n",
              "      <td>0.288649</td>\n",
              "      <td>0.255933</td>\n",
              "      <td>0.582924</td>\n",
              "      <td>0.555552</td>\n",
              "      <td>0.574983</td>\n",
              "      <td>0.574983</td>\n",
              "      <td>0.560987</td>\n",
              "      <td>0.675427</td>\n",
              "      <td>0.615486</td>\n",
              "      <td>0.626019</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0.432074</td>\n",
              "      <td>0.587216</td>\n",
              "      <td>0.619711</td>\n",
              "      <td>0.681905</td>\n",
              "      <td>0.541208</td>\n",
              "      <td>0.541208</td>\n",
              "      <td>0.465453</td>\n",
              "      <td>0.483403</td>\n",
              "      <td>0.588433</td>\n",
              "      <td>0.286784</td>\n",
              "      <td>0.260386</td>\n",
              "      <td>0.588277</td>\n",
              "      <td>0.543051</td>\n",
              "      <td>0.573261</td>\n",
              "      <td>0.573261</td>\n",
              "      <td>0.556591</td>\n",
              "      <td>0.643054</td>\n",
              "      <td>0.615545</td>\n",
              "      <td>0.636374</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>0.429878</td>\n",
              "      <td>0.282175</td>\n",
              "      <td>0.638090</td>\n",
              "      <td>0.297741</td>\n",
              "      <td>0.538950</td>\n",
              "      <td>0.538950</td>\n",
              "      <td>0.468862</td>\n",
              "      <td>0.518470</td>\n",
              "      <td>0.835544</td>\n",
              "      <td>0.289645</td>\n",
              "      <td>0.257755</td>\n",
              "      <td>0.584616</td>\n",
              "      <td>0.547672</td>\n",
              "      <td>0.574312</td>\n",
              "      <td>0.574312</td>\n",
              "      <td>0.558022</td>\n",
              "      <td>0.628012</td>\n",
              "      <td>0.600284</td>\n",
              "      <td>0.670510</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>0.468294</td>\n",
              "      <td>0.602032</td>\n",
              "      <td>0.621016</td>\n",
              "      <td>0.682996</td>\n",
              "      <td>0.529300</td>\n",
              "      <td>0.529300</td>\n",
              "      <td>0.449027</td>\n",
              "      <td>0.469596</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0.286994</td>\n",
              "      <td>0.253157</td>\n",
              "      <td>0.584602</td>\n",
              "      <td>0.556309</td>\n",
              "      <td>0.572743</td>\n",
              "      <td>0.572743</td>\n",
              "      <td>0.559772</td>\n",
              "      <td>0.670048</td>\n",
              "      <td>0.617177</td>\n",
              "      <td>0.640198</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0.428189</td>\n",
              "      <td>0.285268</td>\n",
              "      <td>0.630830</td>\n",
              "      <td>0.300064</td>\n",
              "      <td>0.537041</td>\n",
              "      <td>0.537041</td>\n",
              "      <td>0.461676</td>\n",
              "      <td>0.492096</td>\n",
              "      <td>0.829782</td>\n",
              "      <td>0.288728</td>\n",
              "      <td>0.257440</td>\n",
              "      <td>0.587936</td>\n",
              "      <td>0.549279</td>\n",
              "      <td>0.572244</td>\n",
              "      <td>0.572244</td>\n",
              "      <td>0.554444</td>\n",
              "      <td>0.623063</td>\n",
              "      <td>0.569159</td>\n",
              "      <td>0.696378</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows  20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27d17579-55d0-41a0-b403-d5f15b5bbcb8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-27d17579-55d0-41a0-b403-d5f15b5bbcb8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-27d17579-55d0-41a0-b403-d5f15b5bbcb8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a4449aa5-3167-4700-bcf4-bf718d98ab2a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4449aa5-3167-4700-bcf4-bf718d98ab2a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a4449aa5-3167-4700-bcf4-bf718d98ab2a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_31f0324d-05da-4d36-9e4d-5b89f0958086\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('BENIGN_TEST_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_31f0324d-05da-4d36-9e4d-5b89f0958086 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('BENIGN_TEST_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "BENIGN_TEST_test",
              "summary": "{\n  \"name\": \"BENIGN_TEST_test\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"0_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029166861733609738,\n        \"min\": 0.2357484194335468,\n        \"max\": 0.8228836118599179,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.4703725926892455,\n          0.5033008320821766,\n          0.4076958534720328\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09153257673127807,\n        \"min\": 0.0851611625602538,\n        \"max\": 0.7115962941026452,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.3165041976907671,\n          0.3638709350853855,\n          0.3871111887874781\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07359650406671556,\n        \"min\": 0.0081025986732218,\n        \"max\": 0.89818031384271,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.6495018716616017,\n          0.4627964746062647,\n          0.539265329899089\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0992879635361523,\n        \"min\": 0.1114825163550997,\n        \"max\": 0.7804644502931053,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.3753422074332814,\n          0.3901907192107585,\n          0.3745749703987841\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02561255731505354,\n        \"min\": 0.2230004362941963,\n        \"max\": 0.7785806126633357,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.5696691662759712,\n          0.5913885687164948,\n          0.492523200803237\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02561255731505354,\n        \"min\": 0.2230004362941963,\n        \"max\": 0.7785806126633357,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.5696691662759712,\n          0.5913885687164948,\n          0.492523200803237\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04247766358333103,\n        \"min\": 0.0560106861374997,\n        \"max\": 0.8618078773901475,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.5284519242555697,\n          0.5521555083179035,\n          0.4167951309651493\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06896486419952286,\n        \"min\": 0.0844083909548896,\n        \"max\": 0.8726690921051887,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.7617244800919352,\n          0.504770021631595,\n          0.5424150260372841\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08915473024320808,\n        \"min\": 0.1015273853672966,\n        \"max\": 0.9233024590363323,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.6137121590131914,\n          0.4735709575103496,\n          0.60012425860801\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_pPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012482549925509331,\n        \"min\": 0.2320127958577844,\n        \"max\": 0.4900340912405125,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.3041757229224003,\n          0.2996652908458031,\n          0.2628901406451142\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_tPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.028789092963966624,\n        \"min\": 0.1836832990585606,\n        \"max\": 0.3884069742228974,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.2706386316092185,\n          0.334431111735477,\n          0.2438758408968237\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_rPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07412779823841148,\n        \"min\": 0.3456038975234703,\n        \"max\": 0.6691443306425998,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.4743298217907296,\n          0.4472695619014699,\n          0.4449179026503569\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_sPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.030534901193978455,\n        \"min\": 0.5311413310330273,\n        \"max\": 0.771357762840525,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.6201202542740962,\n          0.6157247222231255,\n          0.5984521848581728\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qPeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0346405979488525,\n        \"min\": 0.4469228438612617,\n        \"max\": 0.6573082634538816,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.5646345188567768,\n          0.4965028622624728,\n          0.5402752630749199\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0346405979488525,\n        \"min\": 0.4469228438612617,\n        \"max\": 0.6573082634538816,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.5646345188567768,\n          0.4965028622624728,\n          0.5402752630749199\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.040979561745308396,\n        \"min\": 0.4040487057426544,\n        \"max\": 0.645479147921841,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.5426822208763572,\n          0.4636678902491369,\n          0.5082186649187815\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.054058127578861304,\n        \"min\": 0.4469667130609795,\n        \"max\": 0.742494691278918,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.5958168193889399,\n          0.5249002536121378,\n          0.5598244503115926\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07167418459424253,\n        \"min\": 0.3652732985897206,\n        \"max\": 0.6963716353198406,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.4907816497610164,\n          0.4480518035770289,\n          0.4611829473534939\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1_qrs_morph4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05444827035419284,\n        \"min\": 0.4477928462270112,\n        \"max\": 0.7618781505117681,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          0.5756328774482729,\n          0.5524908965450626,\n          0.5467667685438438\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BENIGN_TEST_test.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "zGRl8SjKStM5",
        "outputId": "abc6e518-a37f-4a43-c3ae-be4f4e8a85be",
        "collapsed": true
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0_pPeak         0\n",
              "0_tPeak         0\n",
              "0_rPeak         0\n",
              "0_sPeak         0\n",
              "0_qPeak         0\n",
              "0_qrs_morph0    0\n",
              "0_qrs_morph1    0\n",
              "0_qrs_morph3    0\n",
              "0_qrs_morph4    0\n",
              "1_pPeak         0\n",
              "1_tPeak         0\n",
              "1_rPeak         0\n",
              "1_sPeak         0\n",
              "1_qPeak         0\n",
              "1_qrs_morph0    0\n",
              "1_qrs_morph1    0\n",
              "1_qrs_morph2    0\n",
              "1_qrs_morph3    0\n",
              "1_qrs_morph4    0\n",
              "Label           0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_pPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_tPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_rPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_sPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qPeak</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_qrs_morph4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_BENIGN_TEST_test = BENIGN_TEST_test.drop(labels = ['Label'], axis=1)"
      ],
      "metadata": {
        "id": "gXpwC8kcTOgm"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_BENIGN_TEST_test = X_BENIGN_TEST_test.to_numpy()"
      ],
      "metadata": {
        "id": "SdyqixYdTOgm"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_BENIGN_TEST_test = BENIGN_TEST_test['Label']"
      ],
      "metadata": {
        "id": "ISRJ8RIUTOgn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_BENIGN_TEST_test.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "620ab800-92bb-47e7-db95-f95b7a24331e",
        "id": "9t_IHDyqTOgn"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "0    10000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_BENIGN_TEST_test[y_BENIGN_TEST_test>1.0] = 1.0"
      ],
      "metadata": {
        "id": "R6hrq7BTTOgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71939b9e-a022-4ead-d63b-e0028f98fd76"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-89-b1a86c42a65b>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y_BENIGN_TEST_test[y_BENIGN_TEST_test>1.0] = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_BENIGN_TEST_test = y_BENIGN_TEST_test.to_numpy()"
      ],
      "metadata": {
        "id": "-T-8bXNvTOgo"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "3k8maakrn2Cm"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Qju3HMfFk4SC"
      },
      "outputs": [],
      "source": [
        "def construct_pair(X_list_train):  # 3 * 1000 * 29\n",
        "  n_dom = len(X_list_train) # 4 * 1000 * 29\n",
        "  X_in = np.vstack(X_list_train)  ## 3000 * 29 # orignial data with all classes/labels\n",
        "  print(\"Xin SHAPE TRAIN DATA\", X_in.shape)\n",
        "  X_outs = [] # ground truth data #3 * 1000 * 29\n",
        "  for i in range(0, n_dom): # for each domain\n",
        "      X = X_list_train[i]  # take first domain 1 * 1000 * 29\n",
        "      Z_list = []\n",
        "      for j in range(0, n_dom):\n",
        "          Z_list.append(X) # make 3 (num of domains) copies of the same 1000 samples. 3 * 1000 * 29\n",
        "      Z = np.vstack(Z_list) # Z shape: 3000 * 29 # one domain data copied three times\n",
        "      X_outs.append(Z) #  3 * 3000 * 29 ## all class samples stacked to nos of domains.\n",
        "  return X_in, X_outs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "gpYcxOxilc2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c4889f-f725-4b0d-a2a3-83cde41c204e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAPE TRAIN DATA: (4, 4000, 19)\n",
            "Ytrain shape: (16000,)\n",
            "Ytrain : [0 0 0 ... 0 0 0]\n",
            "Xin SHAPE TRAIN DATA (16000, 19)\n",
            "X_in SHAPE: (16000, 19)\n",
            "X_out SHAPE: 4\n"
          ]
        }
      ],
      "source": [
        "X_list_train = data_final_train\n",
        "print (\"SHAPE TRAIN DATA:\", X_list_train.shape)\n",
        "\n",
        "Y_train = labels\n",
        "print (\"Ytrain shape:\", Y_train.shape)\n",
        "print (\"Ytrain :\", Y_train)\n",
        "\n",
        "X_in, X_outs = construct_pair(X_list_train)\n",
        "\n",
        "print (\"X_in SHAPE:\", X_in.shape) # 20000 * 29 # stacked original data\n",
        "print (\"X_out SHAPE:\", len(X_outs)) # 4 * 20000 * 29\n",
        "\n",
        "normed = X_in"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_list_train), len(X_outs), X_list_train[0].shape, X_outs[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwcgONGiR8hQ",
        "outputId": "2da12b96-6c05-4e51-8016-001facef7cb4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 4 (4000, 19) (16000, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Bxi6PluAlRzn"
      },
      "outputs": [],
      "source": [
        "class Dataset(dataset):\n",
        "    def __init__(self, train=True, dom=0):\n",
        "        super(Dataset, self).__init__()\n",
        "\n",
        "        self.dom = dom\n",
        "\n",
        "        if train:\n",
        "            self.inputs = normed  # inputs of all domains\n",
        "            self.outs = X_outs[self.dom]  # the matrix of replicated data sets taken from the lth domain # 1 * 20000 * 29\n",
        "            self.targets = Y_train\n",
        "            self.dom0 = X_outs[0]\n",
        "            self.dom1 = X_outs[1]\n",
        "            self.dom2 = X_outs[2]\n",
        "            self.dom3 = X_outs[3]\n",
        "            #self.dom4 = X_outs[4]\n",
        "            # self.dom = []\n",
        "            # for i in range(len(X_list_train)):\n",
        "            #  self.dom.append(X_list_train[i])\n",
        "        else:\n",
        "            (self.inputs, self.outs) = (X_test_in4, y_test_in4)\n",
        "\n",
        "        # self.images = self.images.reshape(-1, 1, 256)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input = self.inputs[index]\n",
        "        output = self.outs[index]\n",
        "        targets = self.targets[index]\n",
        "        dominp = (self.dom0[index], self.dom1[index], self.dom2[index], self.dom3[index]) #self.dom4[index])\n",
        "        #dominp = tuple(self.dom)\n",
        "        return input, output, targets, dominp  ## input 3000 * 29 , output 1 * 3000 * 29\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "b5zlt1t_n766"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_size = 200\n",
        "feats = 19\n",
        "domains = 4\n",
        "classes = 2\n",
        "latent_dims = 8\n",
        "learning_rate = 0.006"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "cgh5rmAOY1Wz"
      },
      "outputs": [],
      "source": [
        "#model = Autoencoder().to(device)\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Hmp_6D42oATg"
      },
      "outputs": [],
      "source": [
        "class MultitaskAutoencoder(nn.Module):\n",
        "    def __init__(self, D_in, H=20, H2=14, latent_dim=7):\n",
        "        # Encoder\n",
        "        super(MultitaskAutoencoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(D_in, H)  # 29 * 20\n",
        "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
        "        self.linear2 = nn.Linear(H, H2)  # 20 * 10\n",
        "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
        "        self.linear3 = nn.Linear(H2, H2)  # 10 * 10\n",
        "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
        "        self.num_class = classes\n",
        "\n",
        "        self.fc1 = nn.Linear(H2, latent_dim)  # 10 * 7\n",
        "\n",
        "        self.classifier = nn.Linear(latent_dim, self.num_class)  # 7 * 3\n",
        "\n",
        "        # classifier\n",
        "        # self.classifier = nn.Linear(latent_dim, self.num_class) # 7 * 3\n",
        "\n",
        "        #         # Decoder\n",
        "        self.fc3 = nn.Linear(latent_dim, latent_dim)  # 7 * 7\n",
        "        #         self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
        "        self.fc4 = nn.Linear(latent_dim, H2)  # 7 * 10\n",
        "        #         self.fc_bn4 = nn.BatchNorm1d(H2)\n",
        "\n",
        "        self.linear4 = nn.Linear(H2, H2)  # 10 * 10\n",
        "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
        "        self.linear5 = nn.Linear(H2, H)  # 10 * 20\n",
        "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
        "        self.linear6 = nn.Linear(H, D_in)  # 20 * 29\n",
        "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def encode(self, x):\n",
        "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))  # 29 * 20\n",
        "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))  # 20 * 10\n",
        "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))  # 10 * 10\n",
        "\n",
        "        fc1 = self.relu(self.fc1(lin3))  # 10 * 7\n",
        "        # fc2 = F.relu(self.classifier(fc1)) # 7 * 3\n",
        "        return fc1\n",
        "\n",
        "    def decode(self, z):\n",
        "        fc3 = self.relu(self.fc3(z))  # 7 * 7\n",
        "        fc4 = self.relu(self.fc4(fc3))  # .view(128, -1) # 7 * 10\n",
        "\n",
        "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))  # 10 * 10\n",
        "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))  # 10 * 20\n",
        "        return self.lin_bn6(self.linear6(lin5))  # 20 * 29\n",
        "\n",
        "    def forward(self, inputs):  #batch * feats\n",
        "        z = self.encode(inputs)  # 29 * 3\n",
        "        logits = self.classifier(z)\n",
        "        reconstruction = self.decode(z)\n",
        "\n",
        "        return logits, reconstruction, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "SZvgwAbKXnPt"
      },
      "outputs": [],
      "source": [
        "class customLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(customLoss, self).__init__()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.classification_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # def calculate_gram_mat(self,X, sigma):  # required only for codes\n",
        "    #     \"\"\"calculate gram matrix for variables x\n",
        "    #         Args:\n",
        "    #         x: random variable with two dimensional (N,d).\n",
        "    #         sigma: kernel size of x (Gaussain kernel)\n",
        "    #     Returns:\n",
        "    #         Gram matrix (N,N)\n",
        "    #     \"\"\"\n",
        "    #     x = X.view(X.shape[0], -1)\n",
        "    #     instances_norm = torch.sum(x ** 2, -1).reshape((-1, 1))\n",
        "    #     dist = -2 * torch.mm(x, x.t()) + instances_norm + instances_norm.t()\n",
        "\n",
        "    #     return torch.exp(-dist / sigma)\n",
        "\n",
        "    # def renyi_entropy(self, code, sigma):  # code is batch * latent dim\n",
        "    #     # calculate entropy for single variables x (Eq.(9) in paper)\n",
        "    #     #         Args:\n",
        "    #     #         x: random variable with two dimensional (N,d).\n",
        "    #     #         sigma: kernel size of x (Gaussain kernel)\n",
        "    #     #         alpha:  alpha value of renyi entropy\n",
        "    #     #     Returns:\n",
        "    #     #         renyi alpha entropy of x.\n",
        "\n",
        "    #     alpha = 2  ## Renyi's 2nd order entropya\n",
        "\n",
        "    #     # calculate kernel with new updated sigma\n",
        "    #     code_k = self.calculate_gram_mat(code, sigma)\n",
        "    #     code_k = code_k / torch.trace(code_k)\n",
        "    #     # eigv = torch.abs(torch.symeig(k, eigenvectors=True)[0])\n",
        "    #     eigv, eigvec = torch.linalg.eigh(code_k)\n",
        "    #     eig_pow = eigv ** alpha\n",
        "    #     entropy = (1 / (1 - alpha)) * torch.log2(torch.sum(eig_pow))\n",
        "    #     # entropy = -torch.sum(eig_pow)\n",
        "\n",
        "    #     return entropy\n",
        "\n",
        "    # def joint_entropy(self,code, prior, s_x, s_y):  # x = code (batch * feats), y = prior kernel (bacth * batch)\n",
        "\n",
        "    #     \"\"\"calculate joint entropy for random variable x and y (Eq.(10) in paper)\n",
        "    #         Args:\n",
        "    #         x: random variable with two dimensional (N,d).\n",
        "    #         y: random variable with two dimensional (N,d).\n",
        "    #         s_x: kernel size of x\n",
        "    #         s_y: kernel size of y\n",
        "    #         alpha:  alpha value of renyi entropy\n",
        "    #     Returns:\n",
        "    #         joint entropy of x and y.\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     alpha = 2\n",
        "\n",
        "    #     code_k = self.calculate_gram_mat(code, s_x)\n",
        "    #     prior_k = self.calculate_gram_mat(prior, s_y)\n",
        "    #     # prior_k = calculate_gram_mat(prior, s_y) ## prior latent kernel 100 * 29\n",
        "\n",
        "    #     k = torch.mul(code_k, prior_k)\n",
        "    #     k = k / torch.trace(k)\n",
        "    #     # eigv = torch.abs(torch.symeig(k, eigenvectors=True)[0])\n",
        "    #     eigv, eigvec = torch.linalg.eigh(k)\n",
        "    #     eig_pow = eigv ** alpha\n",
        "    #     entropy = (1 / (1 - alpha)) * torch.log2(torch.sum(eig_pow))\n",
        "    #     # entropy = torch.sum(eig_pow)\n",
        "\n",
        "    #     return entropy\n",
        "\n",
        "    # def entropy_loss(self,latent_code, prior_kernel, normalize):  ## calculate MI # x = code , y = prior\n",
        "\n",
        "    #     \"\"\"calculate Mutual information between random variables x and y\n",
        "    #     Args:\n",
        "    #         x: random variable with two dimensional (N,d).\n",
        "    #         y: random variable with two dimensional (N,d).\n",
        "    #         s_x: kernel size of x\n",
        "    #         s_y: kernel size of y\n",
        "    #         normalize: bool True or False, noramlize value between (0,1)\n",
        "    #     Returns:\n",
        "    #         Mutual information between x and y (scale)\n",
        "\n",
        "    #     \"\"\"\n",
        "    #     # global s_x\n",
        "    #     s_x = 0.5  # code\n",
        "    #     s_y = 0.5  # prior\n",
        "\n",
        "    #     # entropy of code. code is batch * latent dimension\n",
        "    #     Hx = self.renyi_entropy(latent_code, sigma=s_x)\n",
        "\n",
        "    #     # entropy of prior ##For prior, RBF kernel is pre-computed. sigma is not considered\n",
        "    #     Hy = self.renyi_entropy(prior_kernel, sigma=s_y)\n",
        "\n",
        "    #     # joint entropy\n",
        "    #     # Hxy = joint_entropy(x, y, s_x, s_y)\n",
        "    #     Hxy = self.joint_entropy(latent_code, prior_kernel, s_x, s_y)\n",
        "\n",
        "    #     if normalize:\n",
        "    #         # Ixy = Hx + Hy - Hxy\n",
        "    #         Ixy = ((Hx * Hy) / (Hxy * Hxy))\n",
        "    #         Ixy = Ixy / (torch.max(Hx, Hy))\n",
        "    #         #print(\"IXY:\", Ixy)\n",
        "    #         # Ixy = torch.log2(Ixy)\n",
        "\n",
        "    #     else:\n",
        "    #         # Ixy = Hx + Hy - Hxy\n",
        "    #         Ixy = ((Hx * Hy) / (Hxy * Hxy))\n",
        "    #         Ixy = Ixy / (torch.max(Hx, Hy))\n",
        "    #         # Ixy = torch.log2(Ixy)\n",
        "\n",
        "    #     return Ixy\n",
        "\n",
        "    def forward(self, input, x_recon, Z, dom_out, logits, targets):\n",
        "        loss_MSE = self.mse_loss(x_recon, dom_out)\n",
        "\n",
        "        targets = torch.flatten(targets)\n",
        "        #print (\"TARGETS:\",targets)\n",
        "\n",
        "        classification_loss = self.classification_criterion(logits, targets)\n",
        "        print('Classification Loss: ', classification_loss, 'MSE Loss: ', loss_MSE)\n",
        "        #entropy_loss = self.entropy_loss(input,Z, True)\n",
        "        # mmd_loss = self.mmd_two_distribution(input, Z, [1, 5, 10])\n",
        "        # print('MMD_Loss: ', mmd_loss)\n",
        "\n",
        "        return 1*classification_loss + (0.1 * loss_MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "bVUSFpCXqWoy"
      },
      "outputs": [],
      "source": [
        "class MMDLoss():\n",
        "  def compute_pairwise_distances(self, x, y):\n",
        "      \"\"\"Computes the squared pairwise Euclidean distances between x and y.\n",
        "      Args:\n",
        "        x: a tensor of shape [num_x_samples, num_features]\n",
        "        y: a tensor of shape [num_y_samples, num_features]\n",
        "      Returns:\n",
        "        a distance matrix of dimensions [num_x_samples, num_y_samples].\n",
        "      \"\"\"\n",
        "      # Define a function to compute the squared Euclidean norm of a tensor\n",
        "      def norm(x):\n",
        "          return torch.sum(torch.square(x), dim=1)\n",
        "\n",
        "      # Compute the squared pairwise Euclidean distances\n",
        "      return norm(torch.unsqueeze(x, 2) - torch.transpose(y, 0, 1))\n",
        "\n",
        "  def rbf_kernel(self, x, y, sigmas):\n",
        "      \"\"\"\n",
        "      compute the rbf kernel value\n",
        "      :param x: [num_x_samples, num_features]\n",
        "      :param y: [num_y_samples, num_features]\n",
        "      :param sigmas: sigmas need to use\n",
        "      :return: single value of x, y kernel\n",
        "      \"\"\"\n",
        "      beta = 1. / (2. * torch.unsqueeze(sigmas, 1))\n",
        "      dist = self.compute_pairwise_distances(x, y)\n",
        "      dot = -torch.matmul(beta, torch.reshape(dist, (1, -1)))\n",
        "      exp = torch.exp(dot)\n",
        "      return torch.mean(exp, dim=1, keepdim=True)\n",
        "\n",
        "  def mmd_two_distribution(self, source, target, sigmas):\n",
        "      \"\"\"\n",
        "      compute mmd loss between two distributions\n",
        "      :param source: [num_samples, num_features]\n",
        "      :param target: [num_samples, num_features]\n",
        "      :return:\n",
        "      \"\"\"\n",
        "\n",
        "      sigmas = torch.tensor(sigmas).to(device)\n",
        "      xy = self.rbf_kernel(source, target, sigmas)\n",
        "      xx = self.rbf_kernel(source, source, sigmas)\n",
        "      yy = self.rbf_kernel(target, target, sigmas)\n",
        "      return xx + yy - 2 * xy\n",
        "\n",
        "  def MMD_Loss_func(self, num_source, y_true, y_pred, sigmas=None):\n",
        "      \"\"\"\n",
        "      MMD loss of multiple sources\n",
        "      :param num_source: number of source domain\n",
        "      :param sigmas: sigma need to use, default: [1, 5, 10]\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      if sigmas is None:\n",
        "          sigmas = [1, 5, 10]\n",
        "\n",
        "      cost = []\n",
        "\n",
        "      for i in range(num_source):\n",
        "          for j in range(num_source):\n",
        "              domain_i = torch.where(y_true == i)[0]\n",
        "              domain_j = torch.where(y_true == j)[0]\n",
        "              single_res = self.mmd_two_distribution(y_pred[domain_i],\n",
        "                                                y_pred[domain_j],\n",
        "                                                sigmas=sigmas)\n",
        "              cost.append(single_res)\n",
        "      cost = torch.cat(cost)\n",
        "      return torch.mean(cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "6nqES7Ycs-DJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff0aab5-ed30-4d26-a111-bb4820a801b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D_in shape: 19\n"
          ]
        }
      ],
      "source": [
        "D_in = 19\n",
        "print (\"D_in shape:\", D_in)\n",
        "H = 8\n",
        "H2 = 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "diYdmc8prDUN"
      },
      "outputs": [],
      "source": [
        "og_interval = 50\n",
        "val_losses = []\n",
        "train_losses = []\n",
        "\n",
        "multitaskAE = MultitaskAutoencoder(D_in, H, H2).to(device)\n",
        "optimizer = torch.optim.Adam(multitaskAE.parameters(), lr=learning_rate, weight_decay=1e-3 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "VQKajW0IrIC-"
      },
      "outputs": [],
      "source": [
        "def train_AE():\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "            train_loss = 0\n",
        "            losses = []\n",
        "\n",
        "            for domain in range(domains):\n",
        "                dataset = Dataset(dom=domain)\n",
        "                dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "                for input, out, targets, dominp in dataloader:\n",
        "                    # data = data.to(device)\n",
        "\n",
        "\n",
        "                    input = input.float().to(device)\n",
        "                    #print (\"INPUTS SHAPE:\", input.shape)\n",
        "\n",
        "                    dom_out = out.float().to(device) # ground truth data\n",
        "                    #print(\"OUTS SHAPE:\", dom_out.shape)\n",
        "\n",
        "                    targets = targets.long().to(device)\n",
        "                    #print (\"TARGETS IN TRAIN:\", targets)\n",
        "\n",
        "                    for i in range(0, len(dominp)):\n",
        "                      dominp[i] = dominp[i].float().to(device)\n",
        "\n",
        "                    logits, recon_batch, Z = multitaskAE(input) ## model\n",
        "                    # print (\"logits train:\", logits)\n",
        "\n",
        "                    loss_mse = customLoss()\n",
        "                    loss = loss_mse(input, recon_batch, Z, dom_out, logits, targets)\n",
        "\n",
        "                    mmd = MMDLoss()\n",
        "                    mmd_loss = []\n",
        "                    for i in range(0, len(dominp)):\n",
        "                      if i != domain:\n",
        "                        mmd_loss.append(mmd.mmd_two_distribution(Z, multitaskAE.encode(dominp[i]), sigmas = [1, 5, 10]))\n",
        "\n",
        "                    print('MMD Loss: ', torch.mean(torch.cat(mmd_loss)))\n",
        "                    loss += 10*torch.mean(torch.cat(mmd_loss))\n",
        "                    losses.append(loss)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            final_loss = torch.mean(torch.stack(losses))\n",
        "\n",
        "            final_loss.backward()\n",
        "            train_loss += final_loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epochs % 50 == 0:\n",
        "                print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "                    epochs, train_loss / len(dataloader.dataset)))\n",
        "                train_losses.append(train_loss / len(dataloader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "uNQ7hN-3rmCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de9412e9-dc55-4621-b729-af488fe55fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2676, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0065, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0672, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2702, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0286, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2678, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0237, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0351, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2716, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0253, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2708, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0216, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2712, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0188, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0428, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2709, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0890, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2686, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0287, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2720, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2694, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2718, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0805, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2697, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0661, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2690, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0326, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0597, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0722, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2707, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2687, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0839, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2667, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1058, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0693, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0435, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2713, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2691, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0862, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0572, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2722, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2675, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0498, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2675, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0138, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2715, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0671, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2674, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0069, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0547, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0196, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2680, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2718, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0428, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2691, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0139, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0216, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0347, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0478, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0487, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2666, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1019, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0374, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0808, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0399, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0516, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2676, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0703, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2697, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0505, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2694, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0454, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2711, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0769, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0592, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2675, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2725, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0449, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2704, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0609, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2699, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0855, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2696, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0411, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2690, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0602, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2687, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0298, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2680, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0470, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0359, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0973, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2690, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0256, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0770, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0555, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2697, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0825, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2714, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0250, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2694, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0327, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2714, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0535, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0482, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0793, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0467, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0381, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2700, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2688, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0672, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2691, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0286, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2680, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0237, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2687, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0351, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2711, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0253, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2686, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0216, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0188, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0428, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2697, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0890, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2704, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0287, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2732, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2714, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0805, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0661, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2716, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0326, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0597, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2673, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0106, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0722, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2686, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0839, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2697, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1058, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0693, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2686, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0435, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2701, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0862, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0572, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2722, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2674, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0498, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0138, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2698, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0671, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2686, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0547, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2694, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0196, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2736, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0428, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2706, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0133, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0216, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2686, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0347, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0478, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0487, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1019, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0374, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2666, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0808, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2700, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2690, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0399, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0516, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2678, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0703, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2699, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0505, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2705, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0454, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0046, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0769, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0592, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0449, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2696, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0609, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0855, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2700, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0095, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0411, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0602, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2686, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0298, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2687, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0470, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0359, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0973, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2664, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0256, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0770, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0555, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2666, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0825, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2697, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0250, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0327, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2700, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0535, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2687, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0482, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2675, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0793, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2714, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0467, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0381, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2701, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0672, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2673, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0286, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2678, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0237, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2715, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0351, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2726, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0253, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2687, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0216, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2717, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0188, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0428, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2712, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0890, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2701, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0287, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2721, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2705, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0805, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0661, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2688, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2688, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0326, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0597, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0158, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0722, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2701, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0839, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2690, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1058, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2674, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0693, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2704, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0435, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2695, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2673, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2676, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0862, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2689, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0572, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2713, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0498, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0138, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2716, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0671, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0048, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0547, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2694, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0196, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2726, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0428, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2702, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0098, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0216, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2675, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0347, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0478, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2680, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0487, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2680, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1019, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0374, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2673, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2691, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0808, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2702, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2687, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0399, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0516, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0703, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2694, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0505, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2710, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0454, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2715, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0769, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0592, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2709, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0449, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2696, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0609, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2696, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0855, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2698, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0071, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0411, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2695, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0602, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0298, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0470, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0359, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0973, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2702, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0256, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2690, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0770, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2715, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0555, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0825, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2708, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0250, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2680, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0327, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2688, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0535, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0482, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2678, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0793, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2697, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0467, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0381, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2686, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0672, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2677, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0286, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2673, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0237, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2691, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0351, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2723, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0253, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2706, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0216, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2705, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0041, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0188, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2685, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0428, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2699, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0050, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0890, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0287, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2711, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0059, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2701, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "====> Epoch: 100 Average loss: 0.0000\n",
            "Classification Loss:  tensor(0.0798, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0657, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0248, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0600, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0171, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0708, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0455, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0834, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2666, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0689, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0654, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0053, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0379, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0848, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0488, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0137, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0665, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0542, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0477, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0046, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0207, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0349, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2674, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1016, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0580, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0449, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0394, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0510, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0705, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2625, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0501, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0447, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0752, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2629, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0863, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0411, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0591, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0361, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0962, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0266, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0769, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0544, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0826, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0247, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0426, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0322, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0526, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0478, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0782, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0468, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0234, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0064, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0677, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0229, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0354, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0741, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0217, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0188, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0895, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0608, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0798, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0657, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0248, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0600, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0708, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0455, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0834, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2630, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0689, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0654, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0379, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2667, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0848, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0488, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2666, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0137, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0665, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0069, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0542, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0477, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2631, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0138, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0207, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0349, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1016, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0580, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0449, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0394, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0510, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0705, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0501, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0447, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0752, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0863, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0411, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0591, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0361, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0962, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0266, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0769, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0544, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2676, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0826, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0247, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0426, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0322, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0526, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0478, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2667, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0782, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0468, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0234, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0677, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0229, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2629, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0354, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0741, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0217, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0188, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0895, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2675, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0608, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0798, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0657, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0248, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0600, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2677, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0106, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0708, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2630, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0455, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0834, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0689, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0654, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0046, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0379, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0848, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2632, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0488, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0137, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0665, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0542, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0477, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0132, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0207, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0349, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1016, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0580, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0449, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0394, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0510, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0705, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0501, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0447, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0046, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0752, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2621, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2632, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0863, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0095, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0411, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0591, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0361, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0962, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0266, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0769, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0544, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0826, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0247, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0426, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0322, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0526, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2667, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0478, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0782, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0468, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0234, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0677, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2630, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0229, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0354, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2632, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0741, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0217, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0188, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0895, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0608, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0798, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2629, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0657, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0248, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0600, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2675, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0157, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0708, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0455, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2674, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0834, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0689, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0654, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0379, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0848, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0488, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0137, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0665, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0048, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0542, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0477, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0098, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0207, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0349, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1016, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0580, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0449, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0394, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0510, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2666, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0705, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0501, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0447, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2664, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0752, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0863, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0071, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0411, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0591, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2667, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0361, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0962, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0266, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0769, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0544, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0826, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0247, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0426, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0322, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0526, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0478, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0782, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0468, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0234, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0078, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0677, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0229, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0354, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0254, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2631, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0741, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0217, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0041, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0188, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0050, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0895, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2630, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0608, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0059, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "====> Epoch: 100 Average loss: 0.0000\n",
            "Classification Loss:  tensor(0.0791, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0377, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0244, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0319, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0601, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0395, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0171, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0706, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0829, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0676, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0650, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0052, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0834, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2630, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0486, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2626, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0135, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0660, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0537, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0191, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0422, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0202, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0346, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0476, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1017, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0373, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0564, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0810, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0440, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0395, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0502, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0707, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2632, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0493, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0739, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0573, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0357, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0446, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0606, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0867, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0410, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0276, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0483, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0358, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0960, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0267, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2631, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0243, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0762, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0827, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0245, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0418, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0322, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0519, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0473, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0772, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0466, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0063, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0679, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2632, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0355, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0433, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0900, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0288, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0604, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0644, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0791, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0377, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0244, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0319, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0601, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0395, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0706, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0829, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2624, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0676, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0650, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0834, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0486, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0135, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0660, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0537, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2630, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0191, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0422, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0137, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0202, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0346, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2630, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0476, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1017, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0373, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0564, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0810, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0440, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0395, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0502, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0707, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0493, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0739, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0573, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0357, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2664, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0446, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0606, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0867, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0088, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0410, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0276, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0483, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0358, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0960, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0267, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0243, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0762, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0827, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0245, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0418, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0322, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0519, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0473, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0772, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0466, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0679, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0355, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0433, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0900, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2669, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0288, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0604, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0644, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0791, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0377, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0244, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0319, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0601, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0395, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0106, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0706, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0829, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0676, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0650, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0834, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0486, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0135, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0660, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0537, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0191, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0422, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0132, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0202, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0346, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0476, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1017, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0373, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0564, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0810, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0440, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0395, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0502, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2647, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0707, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0493, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0046, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0739, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0573, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0357, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2624, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0446, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0606, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0867, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0094, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0410, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0276, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0483, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2624, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0358, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2632, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0960, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0267, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0243, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0762, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2634, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0827, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0245, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0418, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0322, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0519, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0473, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0772, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0466, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0679, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2631, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0355, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2631, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0433, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0900, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2664, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0288, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0604, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0644, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0791, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0377, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0244, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0319, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0601, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0395, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0157, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0706, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0829, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0676, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0650, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0834, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0486, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0135, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0660, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0049, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0537, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0191, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0422, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0097, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0202, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0346, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0476, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1017, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0373, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0564, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0810, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0440, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0395, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2621, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0502, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0707, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0493, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0739, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2629, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0573, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0357, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0446, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0606, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0867, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0410, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0276, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0483, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0358, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0960, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0267, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2627, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0243, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0762, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2667, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0827, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0245, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0418, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0322, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0519, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0473, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0772, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0466, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0077, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0679, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2631, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2640, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0355, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2631, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0433, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0050, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0900, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0288, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2645, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0604, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2626, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0058, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0644, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2643, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "====> Epoch: 100 Average loss: 0.0000\n",
            "Classification Loss:  tensor(0.0786, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2591, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0659, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0318, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0600, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0399, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2591, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0170, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0717, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2610, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2593, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0825, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2608, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0647, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0051, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0369, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0587, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2586, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2583, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0133, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0055, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0531, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0187, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2625, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0412, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0203, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0336, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0462, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2617, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1016, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0362, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0551, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0807, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2608, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0440, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2614, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0404, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0704, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2587, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2610, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0735, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2593, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0568, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0352, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2595, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2614, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0866, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0409, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0272, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0490, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0352, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2589, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0269, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2587, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0239, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0757, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0526, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0821, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0242, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0413, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2591, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0326, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2613, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0766, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2595, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0228, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2591, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0062, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0678, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0287, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2588, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2595, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2609, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0260, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2609, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0184, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2615, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0903, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0602, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2616, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0786, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0659, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0318, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0600, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0399, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0717, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0825, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2579, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2585, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2613, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0647, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0369, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0587, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2618, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0133, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2598, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0531, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2588, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2586, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0187, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2598, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0412, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0137, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0203, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0336, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2586, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2595, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0462, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1016, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0362, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0551, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0807, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0440, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0404, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0704, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2609, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0735, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0568, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0352, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2618, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0866, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0087, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0409, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2595, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0272, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2593, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0490, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0352, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2593, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0269, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0239, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0757, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2598, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0526, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2621, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0821, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2614, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0242, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0413, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2615, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0326, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2618, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2598, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0766, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2615, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0228, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0678, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0287, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2589, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0260, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2595, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0184, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0903, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2624, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2620, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0602, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2610, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0786, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0659, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2626, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0318, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0600, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0399, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2613, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0106, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0717, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2589, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0825, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0647, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0369, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0587, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2614, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0133, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2598, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0531, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2608, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0187, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2613, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2616, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0412, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0131, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0203, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0336, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2615, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0462, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1016, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0362, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0551, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0807, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0440, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2615, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0404, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2616, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0704, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0046, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0735, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2584, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0568, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0352, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2580, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2589, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0866, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2598, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0093, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0409, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0272, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0490, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2580, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0352, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2588, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2593, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0269, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2584, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0239, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0757, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0526, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2591, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0821, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0242, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0413, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2585, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0326, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2614, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2593, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0766, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2617, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0228, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2613, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0678, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2585, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0287, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2589, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2615, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2619, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0260, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2585, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2591, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2616, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0184, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2615, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0041, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0903, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2619, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0602, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2591, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0043, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0786, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2583, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0659, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2598, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0318, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2598, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0600, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0399, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2610, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0156, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0717, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0825, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2617, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0429, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0647, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0369, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2593, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0587, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0249, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0133, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2614, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0656, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0049, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0531, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0472, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0187, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2606, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0412, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0097, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0203, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2591, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0336, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0462, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1016, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0362, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0551, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0807, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2609, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0440, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0404, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2578, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2610, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0704, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2614, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2625, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0735, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2584, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0568, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0352, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0596, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0866, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0409, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2610, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0272, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0490, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2609, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0352, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0269, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2583, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0239, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0757, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2621, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0526, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0821, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2608, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0242, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0413, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2608, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0326, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2589, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2584, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0766, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2602, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2584, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0043, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0228, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0076, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0678, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2588, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0287, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2587, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2595, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2615, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0260, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2586, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2611, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0184, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0049, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0903, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0602, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2581, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0057, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "====> Epoch: 100 Average loss: 0.0000\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0655, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0238, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0317, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0171, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0715, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2543, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0437, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0819, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0427, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0051, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0362, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0801, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0589, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2547, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2521, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0493, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2516, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0131, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0654, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0056, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0468, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0489, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2554, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0569, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0202, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0323, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0455, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2551, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1015, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0544, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0809, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0442, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2544, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0488, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0698, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2515, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0474, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0424, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2542, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0732, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2523, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0561, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0339, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0444, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2521, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0589, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2546, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0871, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2540, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0571, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0268, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0345, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0959, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0273, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2520, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0234, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0751, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2540, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0807, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2542, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0237, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0508, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0462, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2547, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0752, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0453, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2521, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0061, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0678, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0343, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2544, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0224, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0181, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0437, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2549, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0904, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0286, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0636, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2547, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0655, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0238, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0317, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2542, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0715, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0437, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0819, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2512, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2518, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0427, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2548, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0362, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2542, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0801, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0589, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2552, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0493, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0131, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0654, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0071, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0468, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2519, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0489, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0569, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2523, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0137, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0202, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0323, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2518, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0455, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1015, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0544, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0809, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0442, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0488, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0698, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0474, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0424, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0732, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0561, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0339, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2548, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0444, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0589, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2540, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0871, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0087, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0571, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2523, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0268, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0345, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0959, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0273, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0234, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0751, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2551, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0807, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2546, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0237, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2543, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0508, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0462, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0752, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0453, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2548, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0678, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2520, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0343, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0224, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0181, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0437, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0904, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2556, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0286, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2552, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0636, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2544, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0655, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2559, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0238, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0317, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2547, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0107, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0715, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2523, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0437, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0819, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0427, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2542, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0362, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0801, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2520, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0589, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2548, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0493, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0131, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0654, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0468, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2540, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0489, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2543, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0569, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2548, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0131, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0202, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0323, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0455, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1015, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0544, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0809, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2540, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0442, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2546, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0488, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0698, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0474, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2540, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0424, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0047, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0732, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2515, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0561, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0339, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2513, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0444, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0589, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0871, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0093, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0571, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0268, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2513, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0345, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2521, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0959, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0273, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2516, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0234, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0751, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0807, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0237, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2518, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0508, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2542, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0462, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0752, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2549, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0453, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2543, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0678, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2513, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0343, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2553, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2516, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0224, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2546, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0181, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0437, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2549, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0041, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0904, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2550, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0286, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2540, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2514, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0636, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2516, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0655, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0238, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0317, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0156, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0715, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0437, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2544, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0819, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2550, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0427, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0362, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0801, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0589, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2544, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0493, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0131, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2543, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0654, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0050, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0468, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0489, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2543, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0569, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0098, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0202, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2520, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0323, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0455, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1015, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0356, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0544, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0809, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2544, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0442, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2543, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2509, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0488, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0698, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2520, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0474, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2546, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0424, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2554, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0043, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0732, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2515, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0561, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0339, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0444, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2529, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0589, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0871, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0571, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0268, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0345, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0959, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0273, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2516, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0234, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0751, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2551, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0807, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2542, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0237, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2518, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0508, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2515, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0462, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0752, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0453, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2517, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0376, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0043, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0076, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0678, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2517, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0285, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2523, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0343, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2550, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2517, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2544, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0224, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0181, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2531, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0437, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2537, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0049, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0904, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0286, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0057, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0636, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "====> Epoch: 100 Average loss: 0.0000\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0386, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0233, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0316, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0605, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0172, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0709, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0423, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2503, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1050, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0627, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0425, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0646, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0051, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0360, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0798, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0489, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2479, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0129, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0648, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0056, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0511, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0185, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2510, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0568, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2477, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0398, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0198, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0314, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0482, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0454, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2510, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1012, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0542, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0400, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0699, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2476, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0039, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0727, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0553, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0329, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0442, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2473, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0871, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0401, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0567, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0336, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0278, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0228, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0743, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0790, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0231, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0393, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0312, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0503, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0464, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2507, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0735, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0212, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2479, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0061, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0681, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0283, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0338, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0272, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0736, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0177, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2503, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0900, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0622, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0386, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0233, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0316, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0605, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0709, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0423, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2475, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1050, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0627, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0425, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0646, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0360, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2504, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0798, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2504, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0489, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0129, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0648, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0072, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0511, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0185, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0568, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0398, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2480, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0137, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0198, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0314, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2479, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0482, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0454, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1012, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0542, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0400, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0699, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0727, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0553, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0329, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2509, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0442, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0871, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0087, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0401, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0567, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0336, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0278, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0228, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0743, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2514, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0790, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0231, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0393, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2514, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0312, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0503, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0464, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0735, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2507, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0212, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0681, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0283, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2478, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0338, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0272, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0736, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0177, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0900, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2515, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0622, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0386, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2511, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0233, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0316, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0605, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2506, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0107, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0709, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2476, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0423, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1050, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0627, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0425, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0646, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0360, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0798, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2475, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2503, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0489, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0129, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0648, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0511, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0185, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0568, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0398, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0131, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0198, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0314, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0482, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0454, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1012, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0542, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2507, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0400, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2508, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0699, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0047, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0727, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0553, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0329, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2477, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0442, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2479, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0871, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0093, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0401, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0567, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2477, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0336, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0278, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0228, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0743, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0790, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0231, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0393, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0312, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0503, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2506, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0464, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0735, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0212, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0054, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0681, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2475, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0283, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2503, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0338, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0272, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2480, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0736, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2479, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0177, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0041, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0900, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2509, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0622, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0386, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2477, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0233, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0316, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0605, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2506, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0157, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0709, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0423, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2504, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1050, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0627, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2503, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0425, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0646, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0360, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0798, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0489, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0129, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0648, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0051, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0511, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0185, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0484, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0568, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0398, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0098, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0198, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0314, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0482, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0454, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1012, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0542, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0811, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0445, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0400, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2473, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0481, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0699, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0465, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2506, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0407, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2508, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0043, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0727, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0553, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0329, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0442, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0584, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0871, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0401, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0567, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0264, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0494, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2504, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0336, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0278, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0228, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0743, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0790, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0231, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0393, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2507, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0312, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2483, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0503, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0464, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2492, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0735, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0212, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0076, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0681, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2480, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0283, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0338, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0272, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0736, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0220, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2495, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0039, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0177, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0049, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0900, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0289, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0603, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2473, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0057, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0622, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "====> Epoch: 100 Average loss: 0.0000\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0313, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0607, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0389, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0171, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0706, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2403, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0417, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0800, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1044, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0616, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0052, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0357, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0793, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0033, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0255, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0128, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0641, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0056, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0502, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0459, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0185, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2416, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0567, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0310, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0457, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2414, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1010, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0340, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0546, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0810, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0444, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2410, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0474, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0697, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0458, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0393, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0725, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0545, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0320, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0869, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0397, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2407, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0565, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0491, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0327, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0284, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0224, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0505, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2407, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0781, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0381, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0305, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0496, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0715, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0375, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0205, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0061, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0687, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0281, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0335, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2408, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0277, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2407, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0731, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0219, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2408, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0175, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0438, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2412, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0897, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0294, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0599, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0613, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0313, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0607, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2403, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0389, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0706, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0417, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0800, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1044, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0616, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2410, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0357, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0793, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2414, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0255, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0128, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0641, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0072, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0502, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0459, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0185, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0567, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0137, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0310, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0457, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1010, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0340, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0546, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0810, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0444, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0474, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0697, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0458, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0393, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2407, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0725, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0545, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0320, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2416, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0869, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0087, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0397, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0565, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0491, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0327, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0284, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0224, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0505, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2417, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0781, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0381, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2417, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0305, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2415, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0496, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0715, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0375, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2414, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0205, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0687, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0281, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0335, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0277, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0731, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0219, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0175, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0438, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0897, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2421, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0294, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2414, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0599, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0613, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2407, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2420, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0313, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0607, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0389, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2412, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0107, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0706, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0417, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0800, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2408, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1044, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0616, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2403, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0357, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0793, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2411, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0255, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0128, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0641, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0502, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0459, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0185, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0567, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2416, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2403, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0131, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0310, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0457, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1010, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0340, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0546, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0810, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0444, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2412, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2412, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0474, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0697, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0458, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0393, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0047, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0725, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0545, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2403, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0320, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0869, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0093, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0397, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0565, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0491, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0327, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0284, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0224, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0505, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0781, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0381, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0305, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0496, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0715, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2413, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0375, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0205, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0053, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0687, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0281, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2411, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0335, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2417, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0277, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0731, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0219, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2413, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0175, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0438, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2411, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0041, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0897, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2415, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0294, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2403, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0599, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0613, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0785, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0384, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0313, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0607, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0389, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2410, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0157, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0706, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0417, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0800, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1044, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0616, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2412, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0431, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0645, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0357, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0793, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0576, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2407, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0255, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0128, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2411, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0641, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0051, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0502, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0459, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0185, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0567, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2407, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0098, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0310, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0457, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1010, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0340, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0546, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0810, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0444, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0474, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2403, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0697, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0458, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2413, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0393, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2418, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0043, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0725, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0545, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0320, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0441, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0581, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0869, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0397, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0565, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0257, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0491, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2408, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0327, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0961, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0284, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0224, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0740, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2415, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0505, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0781, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0381, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0305, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0496, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0715, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0375, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0205, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0076, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0687, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0281, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0335, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2412, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0277, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0731, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0219, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0039, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0175, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0438, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0049, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0897, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0294, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0599, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0057, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0613, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "====> Epoch: 100 Average loss: 0.0000\n",
            "Classification Loss:  tensor(0.0789, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2366, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2368, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0306, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0613, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2368, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2345, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0170, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0703, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0412, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2348, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0790, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1030, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2358, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2359, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0052, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0351, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0778, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0569, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0261, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2356, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2350, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0127, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0635, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0056, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0561, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0398, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0192, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0309, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0479, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0458, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1008, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0331, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0543, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0438, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0394, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2363, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0471, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2368, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0684, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0452, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0718, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0309, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0580, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0870, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0563, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0485, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2357, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0951, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0291, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2360, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0746, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0498, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0778, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0222, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0374, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2359, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0300, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0456, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0700, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2368, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0368, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2366, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0198, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0061, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0689, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0279, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0227, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0329, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0271, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0730, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0217, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0177, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0447, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0893, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0298, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0597, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0604, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0789, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0306, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0613, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0703, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0412, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2363, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0790, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2356, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1030, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2347, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2361, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0034, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0351, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0778, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0569, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0261, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0127, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0635, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0072, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2358, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0561, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0398, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0136, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0192, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0309, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2353, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0479, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2357, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0458, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2351, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1008, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0331, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2359, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0543, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2368, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0438, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2358, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0394, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2354, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0471, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0684, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0023, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0452, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0718, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0309, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0580, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0870, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0087, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0563, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0485, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0951, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0291, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0746, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0498, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0778, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0222, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0374, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0300, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0456, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2356, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0700, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0368, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0198, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0037, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0689, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0279, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0227, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0329, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0271, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0730, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0217, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0036, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0177, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0031, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0447, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0893, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2395, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0298, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0597, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0604, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0789, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0306, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2366, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0613, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0106, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0703, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2368, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0412, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0790, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1030, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2352, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0351, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0778, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0569, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0261, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0127, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0635, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0561, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0398, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0131, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0192, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0309, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0479, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0458, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1008, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0331, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0543, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0438, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0394, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0471, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0684, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0452, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0046, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0718, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0309, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0580, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0870, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0092, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0563, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0485, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2352, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2354, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0951, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0291, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2355, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0746, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0498, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2358, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0778, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0222, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0374, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2352, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0300, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0456, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2347, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0700, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0368, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0198, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0053, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0689, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0019, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0279, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0227, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0329, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0271, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2368, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0730, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0217, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0177, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2366, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0447, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0041, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0893, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0298, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0597, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0604, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0789, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0378, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2359, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0230, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0306, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0613, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0392, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0156, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0703, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0412, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0790, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1030, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2355, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0610, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0024, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0436, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0640, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2363, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0351, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0778, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0569, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0261, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0475, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0127, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0635, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0051, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0495, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0480, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0561, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0398, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0098, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0192, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0309, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0479, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0458, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2366, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.1008, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0331, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0543, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0438, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0394, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2346, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0008, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0471, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0684, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0452, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0043, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0718, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0534, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0309, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2382, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0005, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0580, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0870, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0563, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0251, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0485, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0321, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2359, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0951, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0291, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2356, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0226, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0746, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0498, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0778, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2383, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0222, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0374, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0009, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0300, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0492, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2357, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0456, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2349, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0700, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0434, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2358, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0368, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0198, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0076, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0689, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0279, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2361, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0227, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0329, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0271, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0730, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0217, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0039, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0177, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0039, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0447, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0048, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0893, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0298, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0597, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0057, grad_fn=<MeanBackward0>)\n",
            "Classification Loss:  tensor(0.0604, grad_fn=<NllLossBackward0>) MSE Loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "MMD Loss:  tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "====> Epoch: 100 Average loss: 0.0000\n",
            "MODEL SAVED\n",
            "TRAIN LOSS: [5.2919752895832064e-05, 5.082255601882934e-05, 4.955160617828369e-05, 4.850489273667336e-05, 4.751265794038773e-05, 4.654185846447945e-05, 4.5559536665678025e-05, 4.456457123160362e-05, 4.355436563491821e-05, 4.2526181787252426e-05, 4.149401932954788e-05, 4.048816114664078e-05, 3.95103320479393e-05, 3.853816166520119e-05, 3.755335137248039e-05, 3.655080869793892e-05, 3.55982817709446e-05, 3.4697093069553376e-05, 3.3840391784906385e-05, 3.304238989949226e-05, 3.227565437555313e-05, 3.1547203660011295e-05, 3.081583045423031e-05, 3.003944084048271e-05, 2.9229819774627684e-05, 2.840106748044491e-05, 2.752353623509407e-05, 2.661333605647087e-05, 2.564789913594723e-05, 2.467154338955879e-05, 2.3715361952781677e-05, 2.2836670279502867e-05, 2.183857187628746e-05, 2.0879242569208146e-05, 1.9915841519832613e-05, 1.9107896834611893e-05, 1.8264727666974067e-05, 1.7467107623815537e-05, 1.667941175401211e-05, 1.5997376292943954e-05, 1.5335356816649437e-05, 1.4399513602256775e-05, 1.41050573438406e-05, 1.3700833544135093e-05, 1.3069966807961464e-05, 1.2738896533846855e-05, 1.2434395961463452e-05, 1.1909997090697289e-05, 1.1659947223961353e-05, 1.1334753595292568e-05, 1.111222803592682e-05, 1.071198470890522e-05, 1.0530918836593627e-05, 1.0335696861147881e-05, 9.980835020542145e-06, 9.809036739170551e-06, 9.60131362080574e-06, 9.396652691066265e-06, 9.207597002387047e-06, 9.112225845456123e-06, 8.937430568039418e-06, 8.86099599301815e-06, 8.776627480983734e-06, 8.746722713112831e-06, 8.617313578724862e-06, 8.509030565619469e-06, 8.387511596083642e-06, 8.260577917098998e-06, 8.148405700922012e-06, 8.039155974984168e-06, 7.927227765321732e-06, 7.811063434928656e-06, 7.603446487337351e-06, 7.551822811365127e-06, 7.5131892226636406e-06, 7.432980928570032e-06, 7.345466408878565e-06, 7.258311845362186e-06, 7.166754454374313e-06, 7.077939808368683e-06, 6.994606461375952e-06, 6.91344728693366e-06, 6.835923995822668e-06, 6.760632153600454e-06, 6.69038062915206e-06, 6.6227233037352564e-06, 6.556336767971516e-06, 6.488046608865261e-06, 6.420955061912537e-06, 6.356765981763601e-06, 6.2917498871684076e-06, 6.221523974090814e-06, 6.133859511464834e-06, 6.0819089412689206e-06, 6.051274482160807e-06, 5.996494088321924e-06, 5.9288307093083854e-06, 5.879697389900684e-06, 5.798297934234142e-06, 5.761799402534962e-06]\n",
            "PLOTTING TRAINING:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "try:\n",
        "    train_AE()\n",
        "    PATH = \"/content/gdrive/My Drive/MTL-RED/MODELS/ARRYTHMIA/MMD_AE/meta_latent_MMD_model_V1.pth\"\n",
        "    torch.save(multitaskAE.state_dict(), PATH)\n",
        "    print(\"MODEL SAVED\")\n",
        "    print (\"TRAIN LOSS:\", train_losses)\n",
        "    print(\"PLOTTING TRAINING:\")\n",
        "    X = np.arange(epochs)\n",
        "    Y = train_losses\n",
        "    plt.plot(X, Y)\n",
        "    plt.savefig('loss_vs_epoch.png')\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # save model\n",
        "    PATH = \"/content/gdrive/My Drive/MTL-RED/MODELS/ARRYTHMIA/MMD_AE/meta_latent_MMD_model_V1.pth\"\n",
        "    torch.save(multitaskAE.state_dict(), PATH)\n",
        "    print(\"MODEL SAVED\")\n",
        "    print(\"PLOTTING TRAINING:\")\n",
        "    X = np.arange(epochs)\n",
        "    Y = train_losses\n",
        "    plt.plot(X, Y)\n",
        "    plt.savefig('loss_vs_epoch.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class DataBuilder(Dataset):\n",
        "    def __init__(self, X_test, y_test):\n",
        "        #self.x, self.standardizer, self.wine = load_data(DATA_PATH)\n",
        "        self.x, self.y = X_test, y_test\n",
        "        self.len=self.x.shape[0]\n",
        "    def __getitem__(self,index):\n",
        "        return self.x[index], self.y[index]\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "J4iPM5WP8b3D"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "AgNXur4y9xiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1ff1f7-f6d3-403e-88e3-0204d0c1af40"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/DATASETS/ARRYTHMIA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "def test(X_test, y_test):\n",
        "  data_set=DataBuilder(X_test, y_test)\n",
        "  testloader=DataLoader(dataset=data_set,batch_size=1)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  predict_lst = []\n",
        "  labels_lst = []\n",
        "\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      multitaskAE.eval()\n",
        "      for data in testloader:\n",
        "          X_test, labels = data\n",
        "          X_test = X_test.float().to(device)\n",
        "          labels = labels.long().to(device)\n",
        "          # calculate outputs by running images through the network\n",
        "          logits, recon_batch, Z = multitaskAE(X_test)\n",
        "          #print (\"logits:\", logits)\n",
        "\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(logits.data, 1)\n",
        "          predict_lst.append(predicted.cpu().detach().numpy())\n",
        "          #predicted = torch.argmax(logits)\n",
        "\n",
        "          labels_lst.append(labels[0].cpu().detach().numpy())\n",
        "\n",
        "          #print(\"out:\", _)\n",
        "\n",
        "          #print (\"PREDICTED:\", predicted)\n",
        "\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f'Accuracy of the network on the test images: {100 * correct / total} %')\n",
        "\n",
        "  # prec = precision_score(labels_lst, predict_lst)\n",
        "  # rec = recall_score(labels_lst, predict_lst)\n",
        "  # f1 = f1_score(labels_lst, predict_lst)\n",
        "  # roc_auc = roc_auc_score(labels_lst, predict_lst)\n",
        "  # print(prec, rec, f1, roc_auc)"
      ],
      "metadata": {
        "id": "yjM__reS918R"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxAnAQ6p8Dnz"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test(np.concatenate((X_BOT_test[:10000], X_BENIGN_TEST_test[:10000])), np.concatenate((y_BOT_test[:10000], y_BENIGN_TEST_test[:10000])))\n",
        "\n",
        "test(np.concatenate((X_BENIGN_TEST_test[:10000], X_BENIGN_TEST_test[:10000])), np.concatenate((y_BENIGN_TEST_test[:10000], y_BENIGN_TEST_test[:10000])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omhf-4eKWNzq",
        "outputId": "09cb67a4-c1b9-4b00-c0b3-03a564362819"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 63.63 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test(np.concatenate((X_RARE_test[:566], X_BENIGN_TEST_test[:566])), np.concatenate((y_RARE_test[:566], y_BENIGN_TEST_test[:566])))\n",
        "test(np.concatenate((X_SVEB_test[:1000], X_SVEB_test[:1000])), np.concatenate((y_SVEB_test[:1000], y_SVEB_test[:1000])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJEZhx9SZU1r",
        "outputId": "e033f443-bbec-40c4-f5d4-6f078560b075"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 44.9 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(np.concatenate((X_VEB_test[:10000], X_VEB_test[:10000])), np.concatenate((y_VEB_test[:10000], y_VEB_test[:10000])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3seX0NBmZVCB",
        "outputId": "b8a4c6f1-9218-496f-c717-5ff93192dae2"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 98.58753031816237 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(np.concatenate((X_F_test[:10000], X_F_test[:10000])), np.concatenate((y_F_test[:10000], y_F_test[:10000])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9iZTEvzZVE0",
        "outputId": "9796776d-6e8b-40bb-ed7f-6cef954822ba"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 89.66376089663761 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test(np.concatenate((X_DDOS_SOLARIS_test[:10000], X_BENIGN_TEST_test[:10000])), np.concatenate((y_DDOS_SOLARIS_test[:10000], y_BENIGN_TEST_test[:10000])))\n",
        "test(np.concatenate((X_Q_test[:10000], X_Q_test[:10000])), np.concatenate((y_Q_test[:10000], y_Q_test[:10000])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t20E2uC4aBLx",
        "outputId": "1b4f48db-94d4-482b-be67-ab66c7e6d4a7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 93.33333333333333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(np.concatenate((X_DDOS_HOIC_test[:10000], X_BENIGN_TEST_test[:10000])), np.concatenate((y_DDOS_HOIC_test[:10000], y_BENIGN_TEST_test[:10000])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "lY_2TVR7aBRn",
        "outputId": "b0d2404b-c945-4f82-9afd-8966321d074b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_DDOS_HOIC_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-0eb4e6b36c8e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_DDOS_HOIC_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_BENIGN_TEST_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_DDOS_HOIC_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_BENIGN_TEST_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_DDOS_HOIC_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(np.concatenate((X_DDOS_GOLDEN_EYE_test[:10000], X_BENIGN_TEST_test[:10000])), np.concatenate((y_DDOS_GOLDEN_EYE_test[:10000], y_BENIGN_TEST_test[:10000])))"
      ],
      "metadata": {
        "id": "8P5ULiRlaTQc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}